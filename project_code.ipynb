{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import math\n",
    "from math import log2\n",
    "import utils\n",
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from collections import defaultdict\n",
    "from vsm import *\n",
    "\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# some helper functions\n",
    "def remove_stop_words(words):\n",
    "    return [word for word in words if not nlp.vocab[word].is_stop]\n",
    "\n",
    "def remove_numeric(words):\n",
    "    return [word for word in words if word.isalpha()]\n",
    "\n",
    "def clean_words(words):\n",
    "    # standardize accents\n",
    "    return [unidecode.unidecode(word) for word in words]\n",
    "    # remove words that are not alpha\n",
    "    #return remove_numeric(words)\n",
    "\n",
    "def normalize(vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:\n",
    "        norm = 1 # avoid division by 0\n",
    "    return vector / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_name = os.getcwd()\n",
    "data_dir_name = \"project_data\"\n",
    "data_dir = os.path.join(base_dir_name, data_dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(signal_filename, relevance_filename, vocab_dict, vocab_id_list, vocab_frequency):\n",
    "    \n",
    "    # We will return these 7 things!\n",
    "    query_dict = {} #maps queries to query id (Assuming distinct queries)\n",
    "    doc_dict = {}  #maps docs to doc id\n",
    "    query_doc_dict = {} #maps query ids to list of doc ids\n",
    "    query_id_list = [] #list\n",
    "    doc_id_list = []\n",
    "    query_doc_relevance = {}\n",
    "    docId_to_content = {} #dict maps from doc id to the contents in the doc. The content is saved as a list of vocab_ids\n",
    "    results_dict = {\n",
    "        'query_dict': query_dict,\n",
    "        'doc_dict': doc_dict,\n",
    "        'query_doc_dict': query_doc_dict,\n",
    "        'query_id_list': query_id_list,\n",
    "        'doc_id_list': doc_id_list,\n",
    "        'query_doc_relevance': query_doc_relevance,\n",
    "        'docId_to_content': docId_to_content,\n",
    "    }\n",
    "    \n",
    "    doc_list_for_query = []\n",
    "    query_repetitions = {} #dict mapping queries to number of repetitions\n",
    "    query_counter = 0\n",
    "    doc_repetitions = 0\n",
    "\n",
    "    with open(os.path.join(data_dir, signal_filename), \"r\", encoding='utf8') as f:\n",
    "        last_query_id = 0 \n",
    "        for line in f:\n",
    "            line_list = line.split()\n",
    "            if line_list[0] == 'query:':\n",
    "                query_counter += 1\n",
    "                if query_counter >= 2:\n",
    "                    query_doc_dict[last_query_id] = doc_list_for_query\n",
    "\n",
    "                query = \" \".join(line_list[1:])\n",
    "\n",
    "                if query_dict.get(query, None) != None:\n",
    "                    query_repetitions[query] = query_repetitions.get(query, 0) + 1\n",
    "                    query = query + \"_\" + str(query_repetitions[query])\n",
    "\n",
    "                query_id_list.append(query)\n",
    "                query_dict[query] = len(query_id_list) - 1\n",
    "\n",
    "                last_query_id = len(query_id_list) - 1 #update the last query whenever a new query starts\n",
    "                doc_list_for_query = [] #reinitialize the doc list whenever a new query starts\n",
    "\n",
    "            elif line_list[0] == 'url:':\n",
    "                assert len(line_list) == 2, \"line_list for url has more than 2 entries. Please check!\"\n",
    "                doc = line_list[1]\n",
    "                if doc_dict.get(doc, None) == None:\n",
    "                    doc_id_list.append(doc)\n",
    "                    doc_id = len(doc_id_list) -1\n",
    "                    doc_dict[doc] = doc_id\n",
    "                else:\n",
    "                    doc_id = doc_dict[doc]\n",
    "                if doc_id not in doc_list_for_query: \n",
    "                    doc_list_for_query.append(doc_id)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        query_doc_dict[last_query_id] = doc_list_for_query\n",
    "\n",
    "\n",
    "    query_total_repetitions = copy.deepcopy(query_repetitions)\n",
    "    doc_relevance_dict = {}\n",
    "    query_counter = 0\n",
    "    with open(os.path.join(data_dir, relevance_filename), \"r\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split()\n",
    "            if line_list[0] == 'query:':\n",
    "                query_counter += 1\n",
    "                query = \" \".join(line_list[1:])\n",
    "                if query_repetitions.get(query, None) != None:\n",
    "                    query_repetition_number = query_total_repetitions[query] - query_repetitions[query]\n",
    "                    query_repetitions[query] -= 1\n",
    "                    if query_repetition_number != 0:\n",
    "                        query = query + \"_\" + str(query_repetition_number)\n",
    "\n",
    "                if query_counter >= 2:\n",
    "                    assert query_doc_relevance.get(last_query_id, None) == None, \"Query already existed in the relevance dict\"\n",
    "                    query_doc_relevance[last_query_id] = doc_relevance_dict\n",
    "\n",
    "                last_query_id = query_dict[query]\n",
    "                doc_relevance_dict = {}\n",
    "\n",
    "            elif line_list[0] == \"url:\":\n",
    "                doc = line_list[1]\n",
    "                docID = doc_dict[doc]\n",
    "\n",
    "                doc_relevance_dict[docID] = float(line_list[-1].strip())\n",
    "\n",
    "        query_doc_relevance[last_query_id] = doc_relevance_dict\n",
    "        \n",
    "    #building word corpus for each document\n",
    "    doc_counter = 0\n",
    "    last_doc_content = defaultdict(list)\n",
    "\n",
    "    with open(os.path.join(data_dir, signal_filename), \"r\", encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split()\n",
    "\n",
    "            if line_list[0] == 'query:':\n",
    "                query = \" \".join(line_list[1:])\n",
    "                queryID = query_dict[query]\n",
    "                for word in line_list[1:]:\n",
    "                    word = word.strip().lower()\n",
    "                    vocab_frequency[word] = vocab_frequency.get(word, 0) + 1\n",
    "\n",
    "                    if vocab_dict.get(word, None) == None:\n",
    "                        vocab_id_list.append(word)\n",
    "                        vocab_dict[word] = len(vocab_id_list) - 1\n",
    "\n",
    "            elif line_list[0] == 'url:':\n",
    "                doc_counter += 1\n",
    "\n",
    "                doc = line_list[1]\n",
    "                docID = doc_dict[doc]\n",
    "\n",
    "                if doc_counter >= 2:\n",
    "                    docId_to_content[last_docID] = last_doc_content\n",
    "\n",
    "                last_doc_content = defaultdict(list)\n",
    "                last_docID = docID\n",
    "\n",
    "            elif line_list[0] == 'title:':\n",
    "                for word in line_list[1:]:\n",
    "                    word = word.strip().lower()\n",
    "                    vocab_frequency[word] = vocab_frequency.get(word, 0) + 1\n",
    "\n",
    "                    if vocab_dict.get(word, None) == None:\n",
    "                        vocab_id_list.append(word)\n",
    "                        vocab_dict[word] = len(vocab_id_list) - 1\n",
    "\n",
    "                    word_id = vocab_dict[word]\n",
    "                    last_doc_content['title'].append(word_id)\n",
    "\n",
    "            elif line_list[0] == 'header:':\n",
    "                for word in line_list[1:]:\n",
    "                    word = word.strip().lower()\n",
    "                    vocab_frequency[word] = vocab_frequency.get(word, 0) + 1\n",
    "\n",
    "                    if vocab_dict.get(word, None) == None:\n",
    "                        vocab_id_list.append(word)\n",
    "                        vocab_dict[word] = len(vocab_id_list) - 1\n",
    "\n",
    "                    word_id = vocab_dict[word]\n",
    "                    last_doc_content['header'].append(word_id)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        docId_to_content[last_docID] = last_doc_content\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab includes words from both, query and docs\n",
    "vocab_dict = {} #mapping from vocab term to id \n",
    "vocab_id_list = [] #list where id maps to the vocab term (0 indexed)\n",
    "vocab_frequency = {} #number of times each vocab term appears in the vocab of documents (included query words)\n",
    "\n",
    "train_dict = load_data(\"pa3.signal.train\", \"pa3.rel.train\", vocab_dict, vocab_id_list, vocab_frequency)\n",
    "dev_dict = load_data(\"pa3.signal.dev\", \"pa3.rel.dev\", vocab_dict, vocab_id_list, vocab_frequency)\n",
    "pkl.dump(dev_dict['doc_id_list'], open('dev_doc_id_list.p', 'wb'))\n",
    "pkl.dump(dev_dict['doc_dict'], open('dev_doc_dict.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing query and document embeddings\n",
    "\n",
    "Document embeddings are obtained from the given title or header information without any weight normalization. Loop through the files and collect doc words by looking in the title and header (one idea can be to give more weight to title than to header). Lookup for each word in the glove embedding. Choose and fix a random combination of word if a word in query does not exist (maybe a combination from the words university and around because the corpus relates to stanford). Ignore otherwise. Finally, find cosine similarity and rank and compute NDCG score. \n",
    "\n",
    "#### Other ideas:\n",
    "1. Treat upper case and start of line word different than end of line word, etc\n",
    "2. Can add word correction, etc\n",
    "3. How scraping documents and adding more words to document effect performance\n",
    "4. Modeling item-item dependency by seq2slate architecture\n",
    "5. Creating embedding for words in the query but not in the embedding vocab as a distinct combination for \n",
    "6. Training word2vec on this and then trying different ideas with the center and context matrices obtained\n",
    "7. DESM type ideas with the embeddings of words in the document weighted by the similarity of words (W_out * q_emb)\n",
    "8. Treating re-ranking task as an NLI task where document entails query\n",
    "9. regressing score for each query-doc pair using nlp inspired regression by predicting score through RNN for instance\n",
    "10. experimenting with listwise and pairwise approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracting away data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data structures we need:\n",
    "    query_dict = {} #maps queries to query id (Assuming distinct queries)\n",
    "    query_id_list = [] #list of queries\n",
    "\n",
    "    doc_dict = {}  #maps urls to doc id\n",
    "    doc_id_list = [] # list of urls\n",
    "\n",
    "    query_doc_dict = {} #maps query ids to list of doc ids\n",
    "    \n",
    "    docId_to_content = {} #dict maps from doc id to the contents in the doc. The content is saved as a list of vocab_ids\n",
    "\n",
    "    #vocab includes words from both, query and docs\n",
    "    vocab_dict = {} #mapping from vocab term to id \n",
    "    vocab_id_list = [] #list where id maps to the vocab term (0 indexed)\n",
    "    vocab_frequency = {} \n",
    "\"\"\"   \n",
    "# dataset_dict is an argument to most of these, \n",
    "# as we need to know if we are dealing with train set, dev set\n",
    "def get_query_string(dataset_dict, query):\n",
    "    if type(query) == int:\n",
    "        query = dataset_dict['query_id_list'][query]\n",
    "    assert type(query) == str, query\n",
    "    return query\n",
    "\n",
    "def get_query_id(dataset_dict, query):\n",
    "    if type(query) == str:\n",
    "        query = dataset_dict['query_dict'][query]\n",
    "    assert type(query) == int, query\n",
    "    return query\n",
    " \n",
    "def get_doc_url(dataset_dict, doc):\n",
    "    if type(doc) == int:\n",
    "        doc = dataset_dict['doc_id_list'][doc]\n",
    "    assert type(doc) == str, doc\n",
    "    return doc\n",
    "\n",
    "def get_doc_id(dataset_dict, doc):\n",
    "    if type(doc) == str:\n",
    "        doc = dataset_dict['doc_dict'][doc]\n",
    "    assert type(doc) == int, doc\n",
    "    return doc\n",
    "\n",
    "# this should be common to everything across train/dev\n",
    "def ids_to_words(content):\n",
    "    # convert ids to words\n",
    "    return [vocab_id_list[i] if type(i) == int else i for i in content]\n",
    "    #return [vocab_id_list[i] for i in content]\n",
    "\n",
    "# this should be common to everything across train/dev\n",
    "def words_to_ids(words):\n",
    "    return [vocab_dict[w] if type(w) == str else w for w in words]\n",
    "    #return [vocab_dict[w] for w in words]\n",
    "\n",
    "# this should be common to everything across train/dev\n",
    "def register_words(words):\n",
    "    for word in words:\n",
    "        if word not in vocab_dict:\n",
    "            vocab_dict[word] = len(vocab_id_list)\n",
    "            vocab_id_list.append(word)\n",
    "\n",
    "def get_query_words(dataset_dict, query):\n",
    "    # return a list of words corresponding to the query (either string query or query_id)\n",
    "    query = get_query_string(dataset_dict, query)\n",
    "    return query.split('_')[0].split(' ')\n",
    "    \n",
    "def get_doc_words(dataset_dict, document, content_type):\n",
    "    # given either url or doc_id\n",
    "    document = get_doc_id(dataset_dict, document)\n",
    "    # all documents have a title at least\n",
    "    doc_to_content = dataset_dict['docId_to_content']\n",
    "    title_content = doc_to_content[document]['title']\n",
    "    \n",
    "    if content_type == 'title':\n",
    "        content = title_content\n",
    "    \n",
    "    elif content_type == 'header':\n",
    "        if doc_to_content[document]['header']:\n",
    "            content = doc_to_content[document]['header']\n",
    "        else:\n",
    "            content = title_content\n",
    "    \n",
    "    elif content_type == '2th':\n",
    "        content = 2*title_content\n",
    "        content += doc_to_content[document]['header']\n",
    "    \n",
    "    elif content_type == 'body':\n",
    "        if doc_to_content[document]['body']:\n",
    "            content = doc_to_content[document]['body']\n",
    "        else:\n",
    "            content = title_content\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid content type: {}\".format(content_type))\n",
    "\n",
    "    return ids_to_words(content)\n",
    "\n",
    "def get_all_doc_words(dataset_dict, query, content_type):\n",
    "    # return tuples of (url, content) corresponding \n",
    "    query = get_query_id(dataset_dict, query)\n",
    "    query_doc_dict = dataset_dict['query_doc_dict']\n",
    "    documents = [get_doc_url(dataset_dict, doc) for doc in query_doc_dict[query]]\n",
    "    return [(doc, get_doc_words(dataset_dict, doc, content_type)) for doc in documents]\n",
    "\n",
    "def get_relevance_dict(dataset_dict, query):\n",
    "    query = get_query_id(dataset_dict, query)\n",
    "    query_doc_relevance = dataset_dict['query_doc_relevance']\n",
    "    return {get_doc_url(dataset_dict, k): v for k, v in query_doc_relevance[query].items()}\n",
    "\n",
    "def query_iter(dataset_dict):\n",
    "    query_dict = dataset_dict['query_dict']\n",
    "    for query in query_dict:\n",
    "        yield query\n",
    "\n",
    "def url_iter(dataset_dict, query):\n",
    "    query_doc_dict = dataset_dict['query_doc_dict']\n",
    "    query = get_query_id(dataset_dict, query)\n",
    "\n",
    "    for doc in query_doc_dict[query]:\n",
    "        yield get_doc_url(dataset_dict, doc)\n",
    "        \n",
    "# Note: from here on out, you NEVER have to touch a datastructure, just use the functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject the body content (by url) into docId_to_content when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary\n",
      "Loading dictionary\n"
     ]
    }
   ],
   "source": [
    "def make_url_to_body(s=\"train\"):\n",
    "    name = \"{}_web_url_to_body.p\".format(s)\n",
    "    if not os.path.exists(name):\n",
    "        print(\"Making dictionary\")\n",
    "        web_vocab_id_list = pkl.load(open('{}_vocab_id_list.p'.format(s), 'rb'))\n",
    "        web_vocab_dict = pkl.load(open('{}_vocab_dict.p'.format(s), 'rb'))\n",
    "        web_docId_to_content = pkl.load(open('{}_doc_id_content.p'.format(s), 'rb'))\n",
    "        web_doc_id_list = pkl.load(open(\"{}_doc_id_list.p\".format(s), \"rb\"))\n",
    "        web_doc_dict = pkl.load(open(\"{}_doc_dict.p\".format(s), \"rb\"))\n",
    "        web_url_to_words = {}\n",
    "\n",
    "        for url, doc in web_doc_dict.items():\n",
    "            body_content = []\n",
    "            if doc in web_docId_to_content:\n",
    "                _, body_content = web_docId_to_content[doc]\n",
    "\n",
    "            body_content = [web_vocab_id_list[w] for w in body_content]\n",
    "            web_url_to_words[url] = body_content\n",
    "\n",
    "        pkl.dump(web_url_to_words, open(name, \"wb\"))\n",
    "\n",
    "    else:\n",
    "        print(\"Loading dictionary\")\n",
    "        web_url_to_words = pkl.load(open(name, \"rb\"))\n",
    "    return web_url_to_words\n",
    "\n",
    "train_web_url_to_words = make_url_to_body('train')\n",
    "dev_web_url_to_words = make_url_to_body('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 0)\n",
      "(1187, 0)\n"
     ]
    }
   ],
   "source": [
    "def fill_in(dataset_dict, web_url_to_words):\n",
    "    present, missing = 0, 0\n",
    "    for query in query_iter(dataset_dict):\n",
    "        for url in url_iter(dataset_dict, query):\n",
    "            if url in web_url_to_words:\n",
    "                content = web_url_to_words[url]\n",
    "                doc_id = get_doc_id(dataset_dict, url)\n",
    "                register_words(content)\n",
    "                content = words_to_ids(content)\n",
    "                docId_to_content = dataset_dict['docId_to_content']\n",
    "                docId_to_content[doc_id]['body'] = content\n",
    "                present += 1\n",
    "            else:\n",
    "                missing += 1\n",
    "    return present, missing\n",
    "\n",
    "print(fill_in(train_dict, train_web_url_to_words))\n",
    "print(fill_in(dev_dict, dev_web_url_to_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stanford', 'aoerc', 'pool', 'hours']\n",
      "\n",
      "[('http://events.stanford.edu/2014/February/18/', ['events', 'at', 'stanford', 'tuesday', 'february', '18', '2014']), ('http://events.stanford.edu/2014/February/6/', ['events', 'at', 'stanford', 'thursday', 'february', '6', '2014']), ('http://events.stanford.edu/2014/March/13/', ['events', 'at', 'stanford', 'thursday', 'march', '13', '2014']), ('http://events.stanford.edu/2014/March/3/', ['events', 'at', 'stanford', 'monday', 'march', '3', '2014']), ('http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf', ['ffw', 'spring', '2017', 'schedule']), ('http://web.stanford.edu/group/masters/pool.html', ['stanford', 'masters', 'swimming', 'pool', '&', 'parking', 'information']), ('https://alumni.stanford.edu/get/page/perks/PoolAndGyms', ['pool', '&', 'gyms']), ('https://cardinalrec.stanford.edu/facilities/aoerc/', []), ('https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=', ['stanford', 'university', 'explore', 'courses']), ('https://glo.stanford.edu/events/stanford-rec-open-house', ['stanford', 'rec', 'open', 'house', 'graduate', 'life', 'office'])]\n",
      "\n",
      "{'http://events.stanford.edu/2014/February/18/': 0.0, 'http://events.stanford.edu/2014/February/6/': 0.0, 'http://events.stanford.edu/2014/March/13/': 0.0, 'http://events.stanford.edu/2014/March/3/': 0.0, 'http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf': 0.0, 'http://web.stanford.edu/group/masters/pool.html': 1.0, 'https://alumni.stanford.edu/get/page/perks/PoolAndGyms': 1.5, 'https://cardinalrec.stanford.edu/facilities/aoerc/': 2.0, 'https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=': 0.5, 'https://glo.stanford.edu/events/stanford-rec-open-house': 0.5}\n",
      "\n",
      "http://events.stanford.edu/2014/February/18/\n",
      "http://events.stanford.edu/2014/February/6/\n",
      "http://events.stanford.edu/2014/March/13/\n",
      "http://events.stanford.edu/2014/March/3/\n",
      "http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf\n",
      "http://web.stanford.edu/group/masters/pool.html\n",
      "https://alumni.stanford.edu/get/page/perks/PoolAndGyms\n",
      "https://cardinalrec.stanford.edu/facilities/aoerc/\n",
      "https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=\n",
      "https://glo.stanford.edu/events/stanford-rec-open-house\n"
     ]
    }
   ],
   "source": [
    "# some sanity checks\n",
    "for query in query_iter(train_dict):\n",
    "    print(get_query_words(train_dict, query))\n",
    "    print()\n",
    "    print(get_all_doc_words(train_dict, query, 'title'))\n",
    "    print()\n",
    "    print(get_relevance_dict(train_dict, query))\n",
    "    print()\n",
    "    for url in url_iter(train_dict, query):\n",
    "        print(url)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lost', 'axess', 'password', 'help']\n",
      "\n",
      "[('https://accounts.stanford.edu/', ['stanford', 'accounts']), ('https://accounts.stanford.edu/resetpw', ['stanford', 'accounts', 'reset', 'password', 'step', '1', 'of', '4']), ('https://uit.stanford.edu/announcements/security', ['announcements', 'university', 'it']), ('https://uit.stanford.edu/service/webauth/twostep', ['two', 'step', 'authentication', 'university', 'it']), ('https://uit.stanford.edu/service/webauth/twostep/bypass_code', ['how', 'to', 'generate', 'a', 'bypass', 'code', 'for', 'a', 'lost', 'or', 'forgotten', 'two', 'step', 'authentication', 'device', 'university', 'it']), ('https://uit.stanford.edu/service/webauth/twostep/printed_list', ['how', 'to', 'use', 'a', 'printed', 'list', 'for', 'two', 'step', 'authentication', 'university', 'it']), ('https://uit.stanford.edu/service/webauth/twostep/push', ['how', 'to', 'authenticate', 'with', 'a', 'duo', 'push', 'notification', 'for', 'two', 'step', 'authentication', 'university', 'it']), ('https://uit.stanford.edu/sites/default/files/2014/08/14/OnGuard%202013%20System%20Administration%20user%20guide.pdf', ['system', 'administration', 'user', 'guide']), ('https://web.stanford.edu/group/coursework/cgi-bin/drupal/?q=node/123', ['coursework', 'release', 'notes', 'coursework', 'help', 'resources'])]\n",
      "\n",
      "{'https://accounts.stanford.edu/': 3.0, 'https://accounts.stanford.edu/resetpw': 1.7, 'https://uit.stanford.edu/announcements/security': 0.3, 'https://uit.stanford.edu/service/webauth/twostep': 1.0, 'https://uit.stanford.edu/service/webauth/twostep/bypass_code': 1.7, 'https://uit.stanford.edu/service/webauth/twostep/printed_list': 1.0, 'https://uit.stanford.edu/service/webauth/twostep/push': 1.0, 'https://uit.stanford.edu/sites/default/files/2014/08/14/OnGuard%202013%20System%20Administration%20user%20guide.pdf': 0.0, 'https://web.stanford.edu/group/coursework/cgi-bin/drupal/?q=node/123': 0.0}\n",
      "\n",
      "https://accounts.stanford.edu/\n",
      "https://accounts.stanford.edu/resetpw\n",
      "https://uit.stanford.edu/announcements/security\n",
      "https://uit.stanford.edu/service/webauth/twostep\n",
      "https://uit.stanford.edu/service/webauth/twostep/bypass_code\n",
      "https://uit.stanford.edu/service/webauth/twostep/printed_list\n",
      "https://uit.stanford.edu/service/webauth/twostep/push\n",
      "https://uit.stanford.edu/sites/default/files/2014/08/14/OnGuard%202013%20System%20Administration%20user%20guide.pdf\n",
      "https://web.stanford.edu/group/coursework/cgi-bin/drupal/?q=node/123\n"
     ]
    }
   ],
   "source": [
    "for query in query_iter(dev_dict):\n",
    "    print(get_query_words(dev_dict, query))\n",
    "    print()\n",
    "    print(get_all_doc_words(dev_dict, query, 'title'))\n",
    "    print()\n",
    "    print(get_relevance_dict(dev_dict, query))\n",
    "    print()\n",
    "    for url in url_iter(dev_dict, query):\n",
    "        print(url)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''having generated query content and doc content, lets try ranking by cosine similarity between query and document \n",
    "embedding\n",
    "'''\n",
    "#iteration 1: ignore words not there in the embedding\n",
    "\n",
    "#lookup function\n",
    "glove_dim = 100\n",
    "GLOVE_HOME = os.path.join('data', 'glove.6B')\n",
    "glove_lookup = utils.glove2dict(os.path.join(GLOVE_HOME, 'glove.6B.{}d.txt'.format(glove_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_embedding(words, combine_func=None, is_query=None):\n",
    "    # is_query is ignored, added to play nice with make_vsm_embedding\n",
    "    for word in words:\n",
    "        assert isinstance(word, str), (type(word), word)\n",
    "\n",
    "    all_vecs = np.array([glove_lookup[w] for w in words if w in glove_lookup]) \n",
    "\n",
    "    if len(all_vecs) == 0:\n",
    "        feats = np.zeros(glove_dim)    \n",
    "    else:       \n",
    "        if combine_func:\n",
    "            feats = combine_func(all_vecs)\n",
    "        else: # take the elemnetwise mean by default\n",
    "            feats = np.mean(all_vecs, axis=0) \n",
    "    return feats\n",
    "\n",
    "def query_and_document_embeddings(dataset_dict, query, query_combine_func=None, doc_combine_func=None, content_type='title', embedding_func=None):\n",
    "    \"\"\"\n",
    "    query: Either query text, or id\n",
    "    query_combine_func: How to combine query GloVe embeddings (default is mean)\n",
    "    doc_combine_func: How to combine document GloVe embeddings (default is mean)\n",
    "    doc_content_type: How to select document content. TODO: make this do something\n",
    "    \"\"\"\n",
    "    if embedding_func is None:\n",
    "        embedding_func = make_glove_embedding\n",
    "        \n",
    "    query_words = get_query_words(dataset_dict, query)\n",
    "    query_embedding = embedding_func(query_words, query_combine_func, is_query=True)\n",
    "        \n",
    "    document_embeddings = [(url, embedding_func(words, doc_combine_func, is_query=False)) \n",
    "                               for url, words in get_all_doc_words(dataset_dict, query, content_type)]\n",
    "    return query_embedding, document_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric - NDCG, MAP\n",
    "\n",
    "can also incorporate Precision, MAP, etc. after binary conversion with decay rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG(ranked_docs, relevance_dict):\n",
    "    '''This function takes an ordered/ranked document list with the ground truth relevance labels from the \n",
    "    relevance_dict and returns a DCG score for the retrieval/ranking.\n",
    "    Input -- \n",
    "        ranked_docs = list of doc IDs ordered by rank. First element in the list is the highest ranked\n",
    "        relevance_dict = dict with keys as the doc_IDs and relevance score as the element\n",
    "    Output -- \n",
    "        DCG [float]'''\n",
    "    return np.sum([(relevance_dict[doc]) / (math.log2(i+2)) \\\n",
    "                  for i, doc in enumerate(ranked_docs)])\n",
    "\n",
    "def DCG_alt(ranked_docs, relevance_dict):\n",
    "    return np.sum([(2**relevance_dict[doc] - 1) / (math.log2(i+2)) \\\n",
    "                   for i, doc in enumerate(ranked_docs)])\n",
    "\n",
    "def NDCG(ranked_docs, relevance_dict, use_alt=False):\n",
    "    '''This function takes an ordered/ranked document list with the ground truth relevance labels from the \n",
    "    relevance_dict and returns a NDCG score for the ranking. \n",
    "    Input -- \n",
    "        ranked_docs = list of doc IDs ordered by rank. First element in the list is the highest ranked\n",
    "        relevance_dict = dict with keys as the doc_IDs and relevance score as the element\n",
    "    Output -- \n",
    "        NDCG [float]'''\n",
    "    assert len(ranked_docs) == len(relevance_dict)\n",
    "    ideal_ordering, _ = zip(*sorted(relevance_dict.items(), key = lambda x: (-x[1])))\n",
    "    ideal_ordering = list(ideal_ordering)\n",
    "    \n",
    "    dcg_func = DCG_alt if use_alt else DCG\n",
    "    DCG_oracle = dcg_func(ideal_ordering, relevance_dict)\n",
    "    DCG_case = dcg_func(ranked_docs, relevance_dict)\n",
    "    assert DCG_oracle >= DCG_case\n",
    "    \n",
    "    #return 0 if DCG_ideal is 0 (happens when all the retrieved docs are rated 0)\n",
    "    if DCG_oracle == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return DCG_case/DCG_oracle\n",
    "\n",
    "#sanity check\n",
    "ranked_docs = [0,1,2,3,4]\n",
    "relevance_dict = {0: 2, 1: 3, 2: 0, 3: 0, 4: 1}\n",
    "DCG_score = 2 + 3/log2(3) + 1/log2(6)\n",
    "Ideal_score = 3 + 2/log2(3) + 1/log2(4)\n",
    "NDCG_score = DCG_score/Ideal_score\n",
    "assert DCG(ranked_docs, relevance_dict) == DCG_score, \"DCG error\"\n",
    "assert NDCG(ranked_docs, relevance_dict) == NDCG_score, \"NDCG error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_helper(relevance_list):\n",
    "    precision = 0.0\n",
    "    relevant_so_far = 0.0\n",
    "    for i, val in enumerate(relevance_list):\n",
    "        relevant_so_far += val\n",
    "        precision += relevant_so_far / (i+1)\n",
    "    return precision / len(relevance_list)\n",
    "\n",
    "def average_precision(ranked_doc_list, query_relevance_dict):\n",
    "    relevance_list = [1 if query_relevance_dict[doc] >= 1.0 else 0 for doc in ranked_doc_list]\n",
    "    return average_precision_helper(relevance_list)\n",
    "\n",
    "expected = (1 + 1 + 2/3 + 2/4 + 3/5 + 3/6 + 4/7)/ 7\n",
    "actual = average_precision_helper([1, 1, 0, 0, 1, 0, 1])\n",
    "assert expected == actual, actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_similarity(doc_embedding, query_embedding):\n",
    "    return np.random.uniform()\n",
    "\n",
    "def cosine_similarity(doc_embedding, query_embedding):\n",
    "    doc_embedding = normalize(doc_embedding)\n",
    "    return np.dot(doc_embedding, query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics(dataset_dict, scoring_func=cosine_similarity, \n",
    "                content_type='title', embedding_func=None, set_name='dev', exp_name='(None)'):\n",
    "    ndcg_sum = 0.0\n",
    "    alt_ndcg_sum = 0.0\n",
    "    precision_sum = 0.0\n",
    "    n = 0\n",
    "    for query in query_iter(dataset_dict):\n",
    "        n += 1\n",
    "        query_relevance_dict = get_relevance_dict(dataset_dict, query)\n",
    "        query_embedding, document_embeddings = \\\n",
    "                query_and_document_embeddings(dataset_dict, query, content_type=content_type, embedding_func=embedding_func)\n",
    "        \n",
    "        scores = [(url, scoring_func(doc_emb, query_embedding)) for url, doc_emb in document_embeddings]\n",
    "        scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "        ranked_doc_list, _ = zip(*scores)\n",
    "        ranked_doc_list = list(ranked_doc_list)\n",
    "\n",
    "        ndcg_sum += NDCG(ranked_doc_list, query_relevance_dict)\n",
    "        alt_ndcg_sum += NDCG(ranked_doc_list, query_relevance_dict, use_alt=True)\n",
    "        precision_sum += average_precision(ranked_doc_list, query_relevance_dict)\n",
    "        \n",
    "    ndcg_sum /= n\n",
    "    alt_ndcg_sum  /= n\n",
    "    precision_sum /= n\n",
    "    return {'NDCG': ndcg_sum,\n",
    "            'Alt_NDCG': alt_ndcg_sum,\n",
    "            'MAP': precision_sum,\n",
    "            'type': set_name,\n",
    "            'name': exp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random ordering accuracy\n",
    "\n",
    "For every query, arrange the docs in random order and check the NDCG value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random on train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.809529579371248,\n",
       " 'Alt_NDCG': 0.7422079090196728,\n",
       " 'MAP': 0.7241636182584593}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "sum_metric = {}\n",
    "for _ in range(n):\n",
    "    for m, v in run_metrics(train_dict, scoring_func=random_similarity).items():\n",
    "        if type(v) != str:\n",
    "            sum_metric[m] = sum_metric.get(m, 0) + v / n\n",
    "print(\"Random on train\")\n",
    "sum_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random on dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8062319836467401,\n",
       " 'Alt_NDCG': 0.7347451765535957,\n",
       " 'MAP': 0.7181887945590422}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "sum_metric = {}\n",
    "for _ in range(n):\n",
    "    for m, v in run_metrics(dev_dict, scoring_func=random_similarity).items():\n",
    "        if type(v) != str:\n",
    "            sum_metric[m] = sum_metric.get(m, 0) + v / n\n",
    "print(\"Random on dev\")\n",
    "sum_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some non-random methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title on train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8651824860113435,\n",
       " 'Alt_NDCG': 0.8161245513899243,\n",
       " 'MAP': 0.7771249238087184,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Title on train\")\n",
    "run_metrics(train_dict, scoring_func=cosine_similarity, content_type='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title on dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8610811796421569,\n",
       " 'Alt_NDCG': 0.8111389290875022,\n",
       " 'MAP': 0.7623313263477439,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Title on dev\")\n",
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header on train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8568503022704564,\n",
       " 'Alt_NDCG': 0.8042551360163497,\n",
       " 'MAP': 0.7661671476556811,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Header on train\")\n",
    "run_metrics(train_dict, scoring_func=cosine_similarity, content_type='header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header on dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8595995880298966,\n",
       " 'Alt_NDCG': 0.8048907651704151,\n",
       " 'MAP': 0.7704660869724235,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Header on dev\")\n",
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*Title+header on train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8685425184105349,\n",
       " 'Alt_NDCG': 0.8196390620542968,\n",
       " 'MAP': 0.778713179048508,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2*Title+header on train\")\n",
    "run_metrics(train_dict, scoring_func=cosine_similarity, content_type='2th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*Title+header on dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8702515039354958,\n",
       " 'Alt_NDCG': 0.8221430739588372,\n",
       " 'MAP': 0.776288219588911,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2*Title+header on dev\")\n",
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body on train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8538332875849136,\n",
       " 'Alt_NDCG': 0.7968008388330325,\n",
       " 'MAP': 0.7690100740266438,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Body on train\")\n",
    "run_metrics(train_dict, scoring_func=cosine_similarity, content_type='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body on dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8706960262890675,\n",
       " 'Alt_NDCG': 0.8121585630178958,\n",
       " 'MAP': 0.7961876798210322,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Body on dev\")\n",
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_featurizer_factory(embedder, content_type=None, combine_func=None, \n",
    "                                  remove_stop=False, combination_type=None):\n",
    "    assert content_type is not None\n",
    "    if combine_func is None:\n",
    "        combine_func = lambda arr: np.mean(arr, axis=0)\n",
    "    assert combination_type in ['concat', 'mult', 'both']\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a function that can be called to featurize data when making a regression dataset\n",
    "    \"\"\"\n",
    "    def featurizer(dataset_dict, query, url):\n",
    "    \n",
    "        query_words = get_query_words(dataset_dict, query)\n",
    "        doc_words = get_doc_words(dataset_dict, url, content_type)\n",
    "\n",
    "        if remove_stop:\n",
    "            query_words = remove_stop_words(query_words)\n",
    "            doc_words = remove_stop_words(doc_words)\n",
    "\n",
    "        query_embedding = embedder(query_words, combine_func, is_query=True)\n",
    "        document_embedding = embedder(doc_words, combine_func, is_query=False) \n",
    "\n",
    "        if combination_type == 'concat':\n",
    "            return np.concatenate([query_embedding, document_embedding])\n",
    "        elif combination_type == 'mult':\n",
    "            return query_embedding * document_embedding\n",
    "        elif combination_type == 'both':\n",
    "            return np.concatenate([query_embedding, document_embedding, query_embedding * document_embedding])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid combination type {}\".format(combination_type))\n",
    "        \n",
    "    return featurizer\n",
    "\n",
    "glove_concat_featurizer_title = regression_featurizer_factory(make_glove_embedding, content_type='title', combination_type='concat')\n",
    "glove_concat_featurizer_header = regression_featurizer_factory(make_glove_embedding, content_type='header', combination_type='concat')\n",
    "glove_concat_featurizer_2_title_header = regression_featurizer_factory(make_glove_embedding, content_type='2th', combination_type='concat')\n",
    "glove_concat_featurizer_body = regression_featurizer_factory(make_glove_embedding, content_type='body', combination_type='concat')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_dataset(dataset_dict, featurizer, repetitive=False):\n",
    "    \"\"\"\n",
    "        dataset_dict: returned by load_data\n",
    "        featurizer: function that takes in (query, doc_content) pair and returns a featurization\n",
    "        \n",
    "        makes a dataset of (vector, relevance) pair where vector is made by the featurizer\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for query in query_iter(dataset_dict): \n",
    "        relevances = get_relevance_dict(dataset_dict, query)\n",
    "        for url in url_iter(dataset_dict, query):\n",
    "            embedding = featurizer(dataset_dict, query, url)\n",
    "            if repetitive:\n",
    "                for emb in embedding: # repetitive means its a list of embeddings, not just one\n",
    "                    X.append(emb)\n",
    "                    y.append(relevances[url])\n",
    "            else:\n",
    "                X.append(embedding)\n",
    "                y.append(relevances[url])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics_ml(dataset_dict, ml_model, featurizer, set_name='dev', exp_name='(None)'):\n",
    "    \"\"\"\n",
    "        dataset_dict: the dictionary returned by load_data\n",
    "        ml_model: a model that takes in X and outputs y predictions\n",
    "        featurizer: the featurizer function that feeds (query, document) \n",
    "            pairs to a format the ml_model can accept\n",
    "    \"\"\"\n",
    "    # TODO: shouldnt have to use the featurizer here, need to improve this API\n",
    "    ndcg_sum = 0.0\n",
    "    alt_ndcg_sum = 0.0\n",
    "    precision_sum = 0.0\n",
    "    n = 0\n",
    "    for query in query_iter(dataset_dict):\n",
    "        n += 1\n",
    "        query_relevance_dict = get_relevance_dict(dataset_dict, query)\n",
    "        \n",
    "        to_rank = [(url, featurizer(dataset_dict, query, url)) for url in url_iter(dataset_dict, query)]\n",
    "        vectors = [vector for _, vector in to_rank]\n",
    "        predictions = ml_model.predict(vectors)\n",
    "        scores = [(url, predictions[i]) for i, (url, _) in enumerate(to_rank)]\n",
    "        \n",
    "        scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "        ranked_doc_list, _ = zip(*scores)\n",
    "        ranked_doc_list = list(ranked_doc_list)\n",
    "\n",
    "        ndcg_sum += NDCG(ranked_doc_list, query_relevance_dict)\n",
    "        alt_ndcg_sum += NDCG(ranked_doc_list, query_relevance_dict, use_alt=True)\n",
    "        precision_sum += average_precision(ranked_doc_list, query_relevance_dict)\n",
    "        \n",
    "    ndcg_sum /= n\n",
    "    alt_ndcg_sum  /= n\n",
    "    precision_sum /= n\n",
    "    return {'NDCG': ndcg_sum,\n",
    "            'Alt_NDCG': alt_ndcg_sum,\n",
    "            'MAP': precision_sum,\n",
    "            'type': set_name,\n",
    "            'name': exp_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_title = make_regression_dataset(train_dict, glove_concat_featurizer_title)\n",
    "#training_dataset_header = make_regression_dataset(train_dict, glove_concat_featurizer_header)\n",
    "#training_dataset_body = make_regression_dataset(train_dict, glove_concat_featurizer_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg:\n",
    "    def __init__(self):\n",
    "        self.model = LinearRegression()\n",
    "        \n",
    "    def train(self, dataset):\n",
    "        X, y = dataset\n",
    "        self.model = self.model.fit(X, y)\n",
    "        return self.model.score(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression with title GloVe embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8542825508514621,\n",
       " 'Alt_NDCG': 0.7973529372338085,\n",
       " 'MAP': 0.7603503995684292,\n",
       " 'type': 'dev',\n",
       " 'name': '(None)'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Linear Regression with title GloVe embeddings\")\n",
    "lin_reg = LinearReg()\n",
    "lin_reg.train(training_dataset_title)\n",
    "run_metrics_ml(dev_dict, lin_reg, glove_concat_featurizer_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_both_featurizer_2_title_header = regression_featurizer_factory(make_glove_embedding, content_type='2th', combination_type='both')\n",
    "training_dataset_2_title_header_poly = make_regression_dataset(train_dict, glove_both_featurizer_2_title_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8594486185683073,\n",
       " 'Alt_NDCG': 0.8044780768696508,\n",
       " 'MAP': 0.7660226025894227,\n",
       " 'type': 'dev',\n",
       " 'name': 'LinReg with GloVe poly embeddings'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearReg()\n",
    "lin_reg.train(training_dataset_2_title_header_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, lin_reg, glove_both_featurizer_2_title_header, set_name='dev', \n",
    "               exp_name='LinReg with GloVe poly embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8741622025809409,\n",
       " 'Alt_NDCG': 0.8255334815413925,\n",
       " 'MAP': 0.7800603010021213,\n",
       " 'type': 'dev',\n",
       " 'name': 'One layer NN with GloVe poly embeddings'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_small = small_neural_net_factory()\n",
    "nn_small.train(training_dataset_2_title_header_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, nn_small, glove_both_featurizer_2_title_header, set_name='dev', \n",
    "               exp_name='One layer NN with GloVe poly embeddings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetReg:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = MLPRegressor(**kwargs)\n",
    "        \n",
    "    def train(self, dataset):\n",
    "        X, y = dataset\n",
    "        self.model = self.model.fit(X, y)\n",
    "        return self.model.score(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_factory():\n",
    "    return NeuralNetReg(hidden_layer_sizes=(100,50), activation='relu', solver='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_neural_net_factory():\n",
    "    return NeuralNetReg(hidden_layer_sizes=(100), activation='relu', solver='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, model_factory, num_models, split=0.8):\n",
    "        self.num_models = num_models\n",
    "        self.split = split\n",
    "        self.models = [model_factory() for _ in range(num_models)]\n",
    "        \n",
    "    def train(self, dataset):\n",
    "        X, y = dataset\n",
    "        n = len(y)\n",
    "        for model_idx in range(self.num_models):\n",
    "            indices = sample_without_replacement(n, n*self.split)\n",
    "            self.models[model_idx].train((X[indices], y[indices]))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # linearly interpolate between all the models\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            predictions += model.predict(X) / self.num_models\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr):\n",
    "    arr = np.exp(arr)\n",
    "    return arr / np.sum(arr)\n",
    "\n",
    "class EnsembleWithEval:\n",
    "    def __init__(self, model_factory, num_models, split=0.8):\n",
    "        assert split < 1.0, split\n",
    "        self.num_models = num_models\n",
    "        self.split = split\n",
    "        self.models = [model_factory() for _ in range(num_models)]\n",
    "        self.evaluations = [None] * num_models\n",
    "        \n",
    "    def train(self, dataset):\n",
    "        X, y = dataset\n",
    "        n = len(y)\n",
    "        for model_idx in range(self.num_models):\n",
    "            indices = sample_without_replacement(n, n*self.split)\n",
    "            eval_indices = [i for i in range(n) if i not in set(indices)]\n",
    "            assert len(eval_indices) > 0\n",
    "            \n",
    "            self.models[model_idx].train((X[indices], y[indices]))\n",
    "            self.evaluate(model_idx, X[eval_indices], y[eval_indices])\n",
    "        \n",
    "        # normalize evaluations\n",
    "        self.evaluations = np.array(self.evaluations) / np.sum(self.evaluations)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # linearly interpolate between all the models based on their evaluations\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model_idx, model in enumerate(self.models):\n",
    "            predictions += model.predict(X) * self.evaluations[model_idx]\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, model_idx, X, y):\n",
    "        self.evaluations[model_idx] = 1. / np.mean(np.square(self.models[model_idx].predict(X) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make VSM embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_co_occurence(dataset_dict):\n",
    "    epsilon = 0.1\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    for query in query_iter(dataset_dict):\n",
    "        query_words = clean_words(get_query_words(dataset_dict, query))\n",
    "        relevance_dict = get_relevance_dict(dataset_dict, query)\n",
    "        for url in url_iter(dataset_dict, query):\n",
    "            title_words = 2 * clean_words(get_doc_words(dataset_dict, url, content_type='title')) # count the title words twice\n",
    "            header_words = clean_words(get_doc_words(dataset_dict, url, content_type='header'))\n",
    "            weight = epsilon + relevance_dict[url]\n",
    "            for q_word in query_words:\n",
    "                for d_word in title_words + header_words:\n",
    "                    counts[q_word][d_word] += weight\n",
    "    return counts\n",
    "\n",
    "def short_print(co_occurence_dict):\n",
    "    n, m = 10, 10\n",
    "    i = 0\n",
    "    for q_word in co_occurence_dict:\n",
    "        i += 1\n",
    "        if i > n:\n",
    "            break\n",
    "        print('\\n' + q_word + ':')\n",
    "\n",
    "        j = 0\n",
    "        for d_word in co_occurence_dict[q_word]:\n",
    "            print(d_word + ',' + str(co_occurence_dict[q_word][d_word]))\n",
    "            j += 1\n",
    "            if j > m:\n",
    "                break\n",
    "        print(len(co_occurence_dict[q_word]))\n",
    "    \n",
    "counts = pd.DataFrame(make_co_occurence(train_dict)).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vsm_title = counts\n",
    "query_vsm_title = counts.copy().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vsm(df, k=100):\n",
    "    return lsa(pmi(df), k).transpose()  # TODO: fix this tranpose business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_embedder_factory(query_df_counts, doc_df_counts, k=100):\n",
    "    query_vsm = transform_vsm(query_df_counts, k)\n",
    "    doc_vsm = transform_vsm(doc_df_counts, k)\n",
    "    \n",
    "    def make_vsm_embedding(words, combine_func=None, is_query=True):\n",
    "        for word in words:\n",
    "            assert isinstance(word, str), (type(word), word)\n",
    "\n",
    "        if is_query:\n",
    "            lookup = query_vsm\n",
    "        else:\n",
    "            lookup = doc_vsm\n",
    "\n",
    "        #for DESM, each vector should be normalized\n",
    "\n",
    "        all_vecs = np.array([normalize(lookup[w]) for w in words if w in lookup]) \n",
    "\n",
    "        if len(all_vecs) == 0:\n",
    "            feats = np.zeros(lookup.values.shape[0])    \n",
    "        else:\n",
    "\n",
    "            if combine_func:\n",
    "                feats = combine_func(all_vecs)\n",
    "            else: # take the elementwise mean by default\n",
    "                feats = np.mean(all_vecs, axis=0)\n",
    "        return feats\n",
    "    \n",
    "    return make_vsm_embedding\n",
    "\n",
    "\n",
    "make_vsm_embedding_title_100 = vsm_embedder_factory(query_vsm_title, doc_vsm_title, k=100)\n",
    "make_vsm_embedding_title_200 = vsm_embedder_factory(query_vsm_title, doc_vsm_title, k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8507632511523792,\n",
       " 'Alt_NDCG': 0.794216545803379,\n",
       " 'MAP': 0.756990611208641,\n",
       " 'type': 'dev',\n",
       " 'name': 'Standard VSM embeddings length 100'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', \n",
    "            embedding_func=make_vsm_embedding_title_100, set_name='dev', exp_name='Standard VSM embeddings length 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8522612530018778,\n",
       " 'Alt_NDCG': 0.7974018981664599,\n",
       " 'MAP': 0.7629867282934678,\n",
       " 'type': 'dev',\n",
       " 'name': 'Standard VSM embeddings length 200'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', \n",
    "            embedding_func=make_vsm_embedding_title_200, set_name='dev', exp_name='Standard VSM embeddings length 200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_concat_featurizer = regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                      combination_type='concat')\n",
    "vsm_mult_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                      combination_type='mult')\n",
    "vsm_poly_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                      combination_type='both')\n",
    "\n",
    "glove_concat_featurizer = regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                      combination_type='concat')\n",
    "glove_mult_featurizer = regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                      combination_type='mult')\n",
    "glove_both_featurizer = regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                      combination_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some analysis to see how many dims we should take from the VSM embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_plot(df, c=10):\n",
    "    rowmat, singvals, colmat = np.linalg.svd(df, full_matrices=False)\n",
    "    plt.plot(range(len(singvals)), singvals)\n",
    "    plt.title(\"Singular values\")\n",
    "    plt.ylabel(\"Singular value\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    \n",
    "    #print the number of eigenvalues to take before the sum is c times the rest\n",
    "    k = 0\n",
    "    while sum(singvals[:k]) < c*sum(singvals[k:]):\n",
    "        k += 1\n",
    "    print(\"Use {} dimensions\".format(k))\n",
    "    \n",
    "    return rowmat, singvals, colmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 168 dimensions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "U, S, Vt = svd_plot(pmi(doc_vsm), c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_tune(df_1, df_2, k_list):\n",
    "    \"\"\"Latent Semantic Analysis using pure scipy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "       The matrix to operate on.\n",
    "    k : int (default: 100)\n",
    "        Number of dimensions to truncate to.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The SVD-reduced version of `df` with dimension (m x k), where\n",
    "        m is the rowcount of mat and `k` is either the user-supplied\n",
    "        k or the column count of `mat`, whichever is smaller.\n",
    "\n",
    "    \"\"\"\n",
    "    rowmat_1, singvals_1, colmat_1 = np.linalg.svd(df_1, full_matrices=False)\n",
    "    rowmat_2, singvals_2, colmat_2 = np.linalg.svd(df_2, full_matrices=False)\n",
    "\n",
    "    singvals_1 = np.diag(singvals_1)\n",
    "    singvals_2 = np.diag(singvals_2)\n",
    "    for k in k_list:\n",
    "        trunc_1 = np.dot(rowmat_1[:, 0:k], singvals_1[0:k, 0:k])\n",
    "        trunc_2 = np.dot(rowmat_2[:, 0:k], singvals_2[0:k, 0:k])\n",
    "\n",
    "        yield (pd.DataFrame(trunc_1, index=df_1.index), pd.DataFrame(trunc_2, index=df_2.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 50\n",
      "{'NDCG': 0.8417815658077111, 'Alt_NDCG': 0.7819818026851135, 'MAP': 0.7474935233950208, 'type': 'dev', 'name': '(None)'}\n",
      "k = 100\n",
      "{'NDCG': 0.8507632511523792, 'Alt_NDCG': 0.794216545803379, 'MAP': 0.756990611208641, 'type': 'dev', 'name': '(None)'}\n",
      "k = 150\n",
      "{'NDCG': 0.8542710777266654, 'Alt_NDCG': 0.7997838787450877, 'MAP': 0.7632202365713312, 'type': 'dev', 'name': '(None)'}\n",
      "k = 200\n",
      "{'NDCG': 0.8522612530018778, 'Alt_NDCG': 0.7974018981664599, 'MAP': 0.7629867282934678, 'type': 'dev', 'name': '(None)'}\n",
      "k = 250\n",
      "{'NDCG': 0.8570996484467451, 'Alt_NDCG': 0.8041851534054328, 'MAP': 0.7664812452758882, 'type': 'dev', 'name': '(None)'}\n",
      "k = 300\n",
      "{'NDCG': 0.8570419234548257, 'Alt_NDCG': 0.804873609491111, 'MAP': 0.7641292014117477, 'type': 'dev', 'name': '(None)'}\n"
     ]
    }
   ],
   "source": [
    "k_list = [50, 100, 150, 200, 250, 300]\n",
    "i = 0\n",
    "for q_vsm, d_vsm in lsa_tune(pmi(query_vsm), pmi(doc_vsm), k_list):\n",
    "    k = k_list[i]\n",
    "    i += 1\n",
    "    q_vsm = q_vsm.transpose()\n",
    "    d_vsm = d_vsm.transpose()\n",
    "    \n",
    "    def tune_vsm_embedding(words, combine_func=None, is_query=True):\n",
    "        for word in words:\n",
    "            assert isinstance(word, str), (type(word), word)\n",
    "\n",
    "        if is_query:\n",
    "            lookup = q_vsm\n",
    "        else:\n",
    "            lookup = d_vsm\n",
    "\n",
    "        #for DESM, each vector should be normalized\n",
    "\n",
    "        all_vecs = np.array([normalize(lookup[w]) for w in words if w in lookup]) \n",
    "\n",
    "        if len(all_vecs) == 0:\n",
    "            feats = np.zeros(lookup.values.shape[0])    \n",
    "        else:\n",
    "\n",
    "            if combine_func:\n",
    "                feats = combine_func(all_vecs)\n",
    "            else: # take the elementwise mean by default\n",
    "                feats = np.mean(all_vecs, axis=0)\n",
    "        return feats\n",
    "\n",
    "    print(\"k =\", k)\n",
    "    print(run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', embedding_func=tune_vsm_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try embedding query words the same, but document words based on co-occurence within all body text (within window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [00:09, 76.19it/s] \n",
      "749it [00:50, 14.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def most_popular(dataset_dict, top=5000):\n",
    "    counts = {}\n",
    "    for query in tqdm(query_iter(dataset_dict)):\n",
    "        for url in url_iter(dataset_dict, query):\n",
    "            body_words = clean_words(get_doc_words(dataset_dict, url, content_type='body'))\n",
    "            for word in body_words:\n",
    "                counts[word] = counts.get(word, 0) + 1\n",
    "    most_popular = sorted([(v, k) for k, v in counts.items()], reverse=True)[:top]\n",
    "    return set([k for v, k in most_popular])\n",
    "\n",
    "def make_window_co_occurence(dataset_dict, k=5):\n",
    "    counts = {}\n",
    "    to_keep = most_popular(dataset_dict) # dont' count words outside of the top 5k\n",
    "    for query in tqdm(query_iter(dataset_dict)):\n",
    "        for url in url_iter(dataset_dict, query):\n",
    "            body_words = [w for w in clean_words(get_doc_words(dataset_dict, url, content_type='body')) if w in to_keep]\n",
    "            for i, first_word in enumerate(body_words):\n",
    "                if first_word not in counts:\n",
    "                    counts[first_word] = {}\n",
    "                for j in range(max(0, i-k), min(i+k+1, len(body_words))):\n",
    "                    second_word = body_words[j]\n",
    "                    if second_word not in counts[first_word]:\n",
    "                        counts[first_word][second_word] = 0\n",
    "                    counts[first_word][second_word] += 1\n",
    "    return counts\n",
    "\n",
    "doc_counts_body  = make_window_co_occurence(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_counts = pd.DataFrame(doc_counts).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 1162 dimensions\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = svd_plot(pmi(doc_counts), c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_body_vsm = doc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8126775220097986,\n",
       " 'Alt_NDCG': 0.7375369838446545,\n",
       " 'MAP': 0.7374502139565505,\n",
       " 'type': 'dev',\n",
       " 'name': 'Embed doc words based on body text'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_vsm_embedding = vsm_embedder_factory(query_vsm, doc_body_vsm, k=200)\n",
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', \n",
    "            embedding_func=make_vsm_embedding, set_name='dev', exp_name='Embed doc words based on body text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_vsm_poly = make_regression_dataset(train_dict, vsm_poly_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8507433299627585,\n",
       " 'Alt_NDCG': 0.789507893885919,\n",
       " 'MAP': 0.7685820197254529,\n",
       " 'type': 'dev',\n",
       " 'name': 'LinReg with VSM poly embeddings from body content'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearReg()\n",
    "lin_reg.train(training_dataset_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, lin_reg, vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='LinReg with VSM poly embeddings from body content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8488608286064229,\n",
       " 'Alt_NDCG': 0.7888354777904943,\n",
       " 'MAP': 0.7644821306658864,\n",
       " 'type': 'dev',\n",
       " 'name': 'LinReg Ensemble with VSM poly embeddings from body content'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_ensemble = Ensemble(LinearReg, 5)\n",
    "lin_reg_ensemble.train(training_dataset_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, lin_reg_ensemble, vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='LinReg Ensemble with VSM poly embeddings from body content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8480981293411232,\n",
       " 'Alt_NDCG': 0.7853457454250327,\n",
       " 'MAP': 0.7649933100480334,\n",
       " 'type': 'dev',\n",
       " 'name': 'LinReg EnsembleWithEval with VSM poly embeddings from body content'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_ensemble = EnsembleWithEval(LinearReg, 5)\n",
    "lin_reg_ensemble.train(training_dataset_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, lin_reg_ensemble, vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='LinReg EnsembleWithEval with VSM poly embeddings from body content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', \n",
    "            embedding_func=make_vsm_embedding, set_name='dev', exp_name='Embed doc words based on body text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try both Glove and VSM embeddings combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_and_vsm_embedding(words, combine_func=None, is_query=True):\n",
    "    vsm = make_vsm_embedding_title_100(words, combine_func=combine_func, is_query=is_query)\n",
    "    glove = make_glove_embedding(words, combine_func=combine_func, is_query=is_query)\n",
    "    return np.concatenate([vsm, glove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8688145224429039,\n",
       " 'Alt_NDCG': 0.820474393827041,\n",
       " 'MAP': 0.7755825744276209,\n",
       " 'type': 'dev',\n",
       " 'name': 'Use both VSM and GloVe embeddings'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(dev_dict, scoring_func=cosine_similarity, content_type='2th', \n",
    "            embedding_func=make_glove_and_vsm_embedding, set_name='dev', exp_name='Use both VSM and GloVe embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_and_vsm_poly_featurizer =  regression_featurizer_factory(make_glove_and_vsm_embedding, content_type='2th', \n",
    "                                                        combination_type='both')\n",
    "training_dataset_glove_vsm_poly = make_regression_dataset(train_dict, glove_and_vsm_poly_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8689434817447704,\n",
       " 'Alt_NDCG': 0.8153732301184206,\n",
       " 'MAP': 0.7852327920415476,\n",
       " 'type': 'dev',\n",
       " 'name': 'LinReg with both GloVe and VSM poly embeddings from body content'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearReg()\n",
    "lin_reg.train(training_dataset_glove_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, lin_reg, glove_and_vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='LinReg with both GloVe and VSM poly embeddings from body content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try training small nn ensembles for Glove, VSM, and Glove + VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8820045023014039,\n",
       " 'Alt_NDCG': 0.8364967144521347,\n",
       " 'MAP': 0.7937512572233191,\n",
       " 'type': 'dev',\n",
       " 'name': 'One layer NN with GloVe poly embeddings from 2th content'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_poly_featurizer =  regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                               combination_type='both')\n",
    "training_dataset_glove_poly = make_regression_dataset(train_dict, glove_poly_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset_glove_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, nn_small_ensemble, glove_poly_featurizer, set_name='dev', \n",
    "               exp_name='One layer NN with GloVe poly embeddings from 2th content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8777398450206124,\n",
       " 'Alt_NDCG': 0.8285844785796038,\n",
       " 'MAP': 0.7845577621729694,\n",
       " 'type': 'dev',\n",
       " 'name': 'One layer NN with VSM poly embeddings from 2th content'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm_poly_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='both')\n",
    "training_dataset_vsm_poly = make_regression_dataset(train_dict, vsm_poly_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, nn_small_ensemble, vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='One layer NN with VSM poly embeddings from 2th content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8906058267364788,\n",
       " 'Alt_NDCG': 0.8483501811247214,\n",
       " 'MAP': 0.7991966723965575,\n",
       " 'type': 'dev',\n",
       " 'name': 'One layer NN with both GloVe and VSM poly embeddings from 2th content'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_and_vsm_poly_featurizer =  regression_featurizer_factory(make_glove_and_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='both')\n",
    "training_dataset_glove_vsm_poly = make_regression_dataset(train_dict, glove_and_vsm_poly_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset_glove_vsm_poly)\n",
    "print(\"trained\")\n",
    "run_metrics_ml(dev_dict, nn_small_ensemble, glove_and_vsm_poly_featurizer, set_name='dev', \n",
    "               exp_name='One layer NN with both GloVe and VSM poly embeddings from 2th content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results (Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "n = 10\n",
    "sum_metric = {}\n",
    "for _ in range(n):\n",
    "    for m, v in run_metrics(train_dict, scoring_func=random_similarity, \n",
    "                            set_name='train', exp_name='Random scoring').items():\n",
    "        if type(v) != str:\n",
    "            sum_metric[m] = sum_metric.get(m, 0) + v / n\n",
    "sum_metric['name'] = 'Random scoring'\n",
    "sum_metric['type'] = 'train'\n",
    "all_metrics.append(sum_metric)\n",
    "\n",
    "sum_metric = {}\n",
    "for _ in range(n):\n",
    "    for m, v in run_metrics(dev_dict, scoring_func=random_similarity, \n",
    "                            set_name='dev', exp_name='Random scoring').items():\n",
    "        if type(v) != str:\n",
    "            sum_metric[m] = sum_metric.get(m, 0) + v / n\n",
    "sum_metric['name'] = 'Random scoring'\n",
    "sum_metric['type'] = 'dev'\n",
    "all_metrics.append(sum_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(train_dict, scoring_func=cosine_similarity, \n",
    "                     content_type='2th', set_name='train', exp_name='Cosine similarity with GloVE embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(dev_dict, scoring_func=cosine_similarity, \n",
    "                     content_type='2th', set_name='dev', exp_name='Cosine similarity with GloVE embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(train_dict, scoring_func=cosine_similarity, \n",
    "                     embedding_func=make_vsm_embedding, set_name='train', exp_name='Cosine similarity with VSM embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(dev_dict, scoring_func=cosine_similarity, \n",
    "                     embedding_func=make_vsm_embedding, set_name='dev', exp_name='Cosine similarity with VSM embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(train_dict, scoring_func=cosine_similarity, \n",
    "                     embedding_func=make_glove_and_vsm_embedding, set_name='train', \n",
    "                     exp_name='Cosine similarity with Glove + VSM embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = run_metrics(dev_dict, scoring_func=cosine_similarity, \n",
    "                     embedding_func=make_glove_and_vsm_embedding, set_name='dev', \n",
    "                     exp_name='Cosine similarity with Glove + VSM embeddings')\n",
    "all_metrics.append(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Small NN ensemble with concat embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_concat_featurizer =  regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                               combination_type='concat')\n",
    "vsm_concat_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='concat')\n",
    "vsm_glove_concat_featurizer =  regression_featurizer_factory(make_glove_and_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, glove_concat_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, glove_concat_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated GloVe embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, glove_concat_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on concatenated GloVe embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, vsm_concat_featurizer)\n",
    "\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, vsm_concat_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, vsm_concat_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on concatenated VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, vsm_glove_concat_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, vsm_glove_concat_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated GloVe + VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, vsm_glove_concat_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on concatenated GloVe + VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small NN ensemble with multiplied embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_mul_featurizer =  regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                               combination_type='mult')\n",
    "vsm_mul_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='mult')\n",
    "glove_and_vsm_mul_featurizer =  regression_featurizer_factory(make_glove_and_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='mult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, glove_mult_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, glove_mul_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on multiplied GloVe embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, glove_mul_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on multiplied GloVe embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, vsm_mul_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, vsm_mul_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on multiplied VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, vsm_mul_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on multiplied VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, glove_and_vsm_mul_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, glove_and_vsm_mul_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on multiplied GloVe and VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, glove_and_vsm_mul_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on multiplied GloVe and VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same but with both concat + mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_both_featurizer =  regression_featurizer_factory(make_glove_embedding, content_type='2th', \n",
    "                                                               combination_type='both')\n",
    "vsm_both_featurizer =  regression_featurizer_factory(make_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='both')\n",
    "glove_and_vsm_both_featurizer =  regression_featurizer_factory(make_glove_and_vsm_embedding, content_type='2th', \n",
    "                                                               combination_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, glove_both_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, glove_both_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated and multiplied GloVe embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, glove_both_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on concatenated and multiplied GloVe embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, vsm_both_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, vsm_both_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated and multiplied VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, vsm_both_featurizer,\n",
    "                        set_name='dev', exp_name='NN ensemble on concatenated and multiplied VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/ssesha/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = make_regression_dataset(train_dict, glove_and_vsm_both_featurizer)\n",
    "nn_small_ensemble = EnsembleWithEval(small_neural_net_factory, 5)\n",
    "nn_small_ensemble.train(training_dataset)\n",
    "\n",
    "metric = run_metrics_ml(train_dict, nn_small_ensemble, glove_and_vsm_both_featurizer, \n",
    "                        set_name='train', exp_name='Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings')\n",
    "all_metrics.append(metric)\n",
    "\n",
    "metric = run_metrics_ml(dev_dict, nn_small_ensemble, glove_and_vsm_both_featurizer,\n",
    "                        set_name='dev', exp_name='Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings')\n",
    "all_metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = pd.DataFrame(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = metric_frame[['name', 'type', 'NDCG', 'Alt_NDCG', 'MAP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>Alt_NDCG</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random scoring</td>\n",
       "      <td>train</td>\n",
       "      <td>0.810559</td>\n",
       "      <td>0.743259</td>\n",
       "      <td>0.726010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random scoring</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.809480</td>\n",
       "      <td>0.739594</td>\n",
       "      <td>0.721148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cosine similarity with GloVE embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.868543</td>\n",
       "      <td>0.819639</td>\n",
       "      <td>0.778713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cosine similarity with GloVE embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.870252</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.776288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cosine similarity with VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.807882</td>\n",
       "      <td>0.741058</td>\n",
       "      <td>0.725608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cosine similarity with VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.824462</td>\n",
       "      <td>0.753089</td>\n",
       "      <td>0.743511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cosine similarity with Glove + VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.865965</td>\n",
       "      <td>0.817066</td>\n",
       "      <td>0.777946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cosine similarity with Glove + VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.860184</td>\n",
       "      <td>0.809570</td>\n",
       "      <td>0.762028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Small NN ensemble on concatenated GloVe embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.990661</td>\n",
       "      <td>0.984391</td>\n",
       "      <td>0.889707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Small NN ensemble on concatenated GloVe embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.889265</td>\n",
       "      <td>0.843126</td>\n",
       "      <td>0.791556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Small NN ensemble on concatenated VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.990618</td>\n",
       "      <td>0.984213</td>\n",
       "      <td>0.889555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Small NN ensemble on concatenated VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.874922</td>\n",
       "      <td>0.825378</td>\n",
       "      <td>0.780052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Small NN ensemble on concatenated GloVe + VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.993877</td>\n",
       "      <td>0.989922</td>\n",
       "      <td>0.890663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Small NN ensemble on concatenated GloVe + VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.886293</td>\n",
       "      <td>0.839834</td>\n",
       "      <td>0.796335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Small NN ensemble on multiplied GloVe embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.967752</td>\n",
       "      <td>0.950512</td>\n",
       "      <td>0.872499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Small NN ensemble on multiplied GloVe embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.863304</td>\n",
       "      <td>0.807880</td>\n",
       "      <td>0.777508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Small NN ensemble on multiplied VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.858635</td>\n",
       "      <td>0.803193</td>\n",
       "      <td>0.777806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Small NN ensemble on multiplied VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.835202</td>\n",
       "      <td>0.772087</td>\n",
       "      <td>0.748177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Small NN ensemble on multiplied GloVe and VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.977206</td>\n",
       "      <td>0.963037</td>\n",
       "      <td>0.879326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Small NN ensemble on multiplied GloVe and VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.872957</td>\n",
       "      <td>0.818852</td>\n",
       "      <td>0.789035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Small NN ensemble on concatenated and multiplied GloVe embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.992684</td>\n",
       "      <td>0.988200</td>\n",
       "      <td>0.889626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Small NN ensemble on concatenated and multiplied GloVe embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.881575</td>\n",
       "      <td>0.837911</td>\n",
       "      <td>0.783180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Small NN ensemble on concatenated and multiplied VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.990696</td>\n",
       "      <td>0.984002</td>\n",
       "      <td>0.889851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NN ensemble on concatenated and multiplied VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.872579</td>\n",
       "      <td>0.820538</td>\n",
       "      <td>0.781442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings</td>\n",
       "      <td>train</td>\n",
       "      <td>0.994739</td>\n",
       "      <td>0.990932</td>\n",
       "      <td>0.892467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings</td>\n",
       "      <td>dev</td>\n",
       "      <td>0.882973</td>\n",
       "      <td>0.838056</td>\n",
       "      <td>0.793326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       name  \\\n",
       "0   Random scoring                                                            \n",
       "1   Random scoring                                                            \n",
       "2   Cosine similarity with GloVE embeddings                                   \n",
       "3   Cosine similarity with GloVE embeddings                                   \n",
       "4   Cosine similarity with VSM embeddings                                     \n",
       "5   Cosine similarity with VSM embeddings                                     \n",
       "6   Cosine similarity with Glove + VSM embeddings                             \n",
       "7   Cosine similarity with Glove + VSM embeddings                             \n",
       "8   Small NN ensemble on concatenated GloVe embeddings                        \n",
       "9   Small NN ensemble on concatenated GloVe embeddings                        \n",
       "10  Small NN ensemble on concatenated VSM embeddings                          \n",
       "11  Small NN ensemble on concatenated VSM embeddings                          \n",
       "12  Small NN ensemble on concatenated GloVe + VSM embeddings                  \n",
       "13  Small NN ensemble on concatenated GloVe + VSM embeddings                  \n",
       "14  Small NN ensemble on multiplied GloVe embeddings                          \n",
       "15  Small NN ensemble on multiplied GloVe embeddings                          \n",
       "16  Small NN ensemble on multiplied VSM embeddings                            \n",
       "17  Small NN ensemble on multiplied VSM embeddings                            \n",
       "18  Small NN ensemble on multiplied GloVe and VSM embeddings                  \n",
       "19  Small NN ensemble on multiplied GloVe and VSM embeddings                  \n",
       "20  Small NN ensemble on concatenated and multiplied GloVe embeddings         \n",
       "21  Small NN ensemble on concatenated and multiplied GloVe embeddings         \n",
       "22  Small NN ensemble on concatenated and multiplied VSM embeddings           \n",
       "23  NN ensemble on concatenated and multiplied VSM embeddings                 \n",
       "24  Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings   \n",
       "25  Small NN ensemble on concatenated and multiplied GloVe + VSM embeddings   \n",
       "\n",
       "     type      NDCG  Alt_NDCG       MAP  \n",
       "0   train  0.810559  0.743259  0.726010  \n",
       "1   dev    0.809480  0.739594  0.721148  \n",
       "2   train  0.868543  0.819639  0.778713  \n",
       "3   dev    0.870252  0.822143  0.776288  \n",
       "4   train  0.807882  0.741058  0.725608  \n",
       "5   dev    0.824462  0.753089  0.743511  \n",
       "6   train  0.865965  0.817066  0.777946  \n",
       "7   dev    0.860184  0.809570  0.762028  \n",
       "8   train  0.990661  0.984391  0.889707  \n",
       "9   dev    0.889265  0.843126  0.791556  \n",
       "10  train  0.990618  0.984213  0.889555  \n",
       "11  dev    0.874922  0.825378  0.780052  \n",
       "12  train  0.993877  0.989922  0.890663  \n",
       "13  dev    0.886293  0.839834  0.796335  \n",
       "14  train  0.967752  0.950512  0.872499  \n",
       "15  dev    0.863304  0.807880  0.777508  \n",
       "16  train  0.858635  0.803193  0.777806  \n",
       "17  dev    0.835202  0.772087  0.748177  \n",
       "18  train  0.977206  0.963037  0.879326  \n",
       "19  dev    0.872957  0.818852  0.789035  \n",
       "20  train  0.992684  0.988200  0.889626  \n",
       "21  dev    0.881575  0.837911  0.783180  \n",
       "22  train  0.990696  0.984002  0.889851  \n",
       "23  dev    0.872579  0.820538  0.781442  \n",
       "24  train  0.994739  0.990932  0.892467  \n",
       "25  dev    0.882973  0.838056  0.793326  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "metric_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
