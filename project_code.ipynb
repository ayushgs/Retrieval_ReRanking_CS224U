{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "from math import log2\n",
    "import utils\n",
    "\n",
    "from base_classes.load_train_data import load_train_data\n",
    "from base_classes.query import Query\n",
    "from base_classes.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa3.rel.train\n",
      "BSBI.dict\n",
      "docs.dict\n",
      "pa3.rel.dev\n",
      "pa3.signal.dev\n",
      "terms.dict\n",
      "pa3.signal.train\n"
     ]
    }
   ],
   "source": [
    "base_dir_name = os.getcwd()\n",
    "data_dir_name = \"project_data\"\n",
    "data_dir = os.path.join(base_dir_name, data_dir_name)\n",
    "\n",
    "for item in os.listdir(data_dir):\n",
    "    if not item.startswith('.') and os.path.isfile(os.path.join(data_dir, item)):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The new plumbing\n",
    "\n",
    "Given a Query object query, query_dict[query] maps to a dictionary url_dict.\n",
    "url_dict maps urls to Documet objects.\n",
    "Documents will store the headers, title, body_content (both as lists of word_ids, which can be mapped to strings)\n",
    "as well as relevance to the query.\n",
    "\n",
    "Note! Do not make Query's from words. Using the query mapping to get a Query obj from words, and words from a Query obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load up the objects from the web scraping, as they define body_content and an initial vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were built by the web scraping process\n",
    "vocab_id_list = pkl.load(open('vocab_id_list.p', 'rb'))\n",
    "vocab_dict = pkl.load(open('vocab_dict.p', 'rb'))\n",
    "vocab_frequency = pkl.load(open('vocab_frequency.p', 'rb'))\n",
    "docId_to_content = pkl.load(open('doc_id_content.p', 'rb'))\n",
    "doc_id_list = pkl.load(open(\"doc_id_list.p\", \"rb\"))\n",
    "doc_dict = pkl.load(open(\"doc_dict.p\", \"rb\"))\n",
    "\n",
    "file_name = os.path.join(data_dir, \"pa3.signal.train\")\n",
    "query_dict = load_train_data(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to map from word_ids (integers) to words (strings) and vice versa \n",
    "class WordIDMap:\n",
    "    def __init__(self, vocab_string_list=None, vocab_dict=None):\n",
    "        self.vocab_string_list = []\n",
    "        self.vocab_dict = {}\n",
    "        if vocab_string_list and vocab_dict:\n",
    "            self.vocab_string_list = vocab_string_list\n",
    "            self.vocab_dict = vocab_dict\n",
    "    \n",
    "    def add_string(self, string):\n",
    "        if string not in self.vocab_dict:\n",
    "            self.vocab_dict[string] = len(self.vocab_string_list)\n",
    "            self.vocab_string_list.append(string)\n",
    "        \n",
    "    def get_string(self, ID):\n",
    "        assert ID < len(self.vocab_string_list)\n",
    "        return self.vocab_string_list[ID]\n",
    "        \n",
    "    def get_id(self, string):\n",
    "        if string not in self.vocab_dict:\n",
    "            self.add_string(string) # this way we don't have to explicitly add query/doc title words to the mapping\n",
    "        return self.vocab_dict[string]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocab_string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanford\n"
     ]
    }
   ],
   "source": [
    "# instantiate the WordIDMap with the words scraped from the stanford domain\n",
    "vocab_id_list = pkl.load(open('vocab_id_list.p', 'rb'))\n",
    "vocab_dict = pkl.load(open('vocab_dict.p', 'rb'))\n",
    "word_map = WordIDMap(vocab_id_list, vocab_dict)\n",
    "\n",
    "before_length = len(word_map)\n",
    "print(word_map.get_string(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the relevance scores from the train set, and add them to the documents within the query_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_scores(query_dict, filename):\n",
    "    urls_missing = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"q\"):\n",
    "                query = line.split(\":\")[-1].strip()\n",
    "                query = Query(query)\n",
    "                \n",
    "                 # assume the query is the one we want (otherwise, would be a duplicate)\n",
    "                query_in_dict = True\n",
    "            else: #urls\n",
    "                tokens = line[line.index(\":\")+1:].strip().split(\" \")\n",
    "                url = tokens[0]\n",
    "                rel = tokens[1]\n",
    "                if float(rel) < 0:\n",
    "                     rel = 0\n",
    "                if url in query_dict[query] and query_in_dict:   \n",
    "                    document = query_dict[query][url]\n",
    "                    document.relevance = float(rel)\n",
    "                else:\n",
    "                    # we know this query is wrong, so wait til a new query comes along\n",
    "                    query_in_dict = False\n",
    "                    urls_missing.append((query,url))\n",
    "                    \n",
    "    return urls_missing\n",
    "\n",
    "def get_relevance_dict(query_dict, query):\n",
    "    return {url: document.relevance for url, document in query_dict[query].items()}\n",
    "             \n",
    "relevance_filenames = os.path.join(data_dir, 'pa3.rel.train')\n",
    "urls_missing = get_rel_scores(query_dict, relevance_filenames)\n",
    "\n",
    "for query, url_dict in query_dict.items():\n",
    "    for url, doc in url_dict.items():\n",
    "        assert doc.relevance is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(cardinal nights, 'http://events.stanford.edu/events/453/45363/'),\n",
       " (cardinal nights, 'https://alcohol.stanford.edu/cardinal-nights'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/cardinal-nights-comedic-guest'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/cardinal-nights-goosebumps-screening'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/cardinal-nights-midnight-premiere-hunger-games-mockingjay-part-1'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/cardinal-nights-mini-grant-casino-night'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/cardinal-nights-pick-your-own-movie-night-0'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/meet-staff-cardinal-nights'),\n",
       " (cardinal nights,\n",
       "  'https://alcohol.stanford.edu/events/outdoor-ed-and-cardinal-nights-presents-dive-movie'),\n",
       " (cardinal nights, 'https://events.stanford.edu/events/618/61821/'),\n",
       " (arrillaga gym hours,\n",
       "  'http://ortho.stanford.edu/lacob/sports-medicine-center.html'),\n",
       " (arrillaga gym hours, 'http://outdoored.stanford.edu/center/'),\n",
       " (arrillaga gym hours,\n",
       "  'https://alumni.stanford.edu/get/page/membership/benefits/pools'),\n",
       " (arrillaga gym hours,\n",
       "  'https://alumni.stanford.edu/get/page/perks/PoolAndGyms'),\n",
       " (arrillaga gym hours, 'https://cardinalatwork.stanford.edu/'),\n",
       " (arrillaga gym hours,\n",
       "  'https://cardinalrec.stanford.edu/facilities/facility-hours/'),\n",
       " (arrillaga gym hours,\n",
       "  'https://med.stanford.edu/content/dam/sm/hip/documents/FitnessAtAGlance.pdf'),\n",
       " (arrillaga gym hours,\n",
       "  'https://med.stanford.edu/content/dam/sm/scalpel/documents/WellBeing/After%20Hours.2015.pdf'),\n",
       " (arrillaga gym hours, 'https://rde.stanford.edu/dining'),\n",
       " (arrillaga gym hours,\n",
       "  'https://web.stanford.edu/group/Taekwondo/documents/rec-center.pdf'),\n",
       " (stanford dining hours,\n",
       "  'https://biox.stanford.edu/about/clark-center/dining'),\n",
       " (stanford dining hours,\n",
       "  'https://rde.stanford.edu/dining/arrillaga-family-dining-commons'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/dining/dining-hall-hours'),\n",
       " (stanford dining hours,\n",
       "  'https://rde.stanford.edu/dining/florence-moore-dining'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/dining/graduate-gateway'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/dining/lakeside-dining'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/dining/late-nite-lakeside'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/dining/locations-hours'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/hospitality/hours-service'),\n",
       " (stanford dining hours, 'https://rde.stanford.edu/hospitality/market-munger'),\n",
       " (aoerc pool hours, 'http://events.stanford.edu/2014/March/7/'),\n",
       " (aoerc pool hours,\n",
       "  'http://explorecourses.stanford.edu/search;jsessionid=qt00fhzmis7x1k7ixkmj6mxpn?q=PE&view=catalog&filter-term-Winter=on&academicYear=&page=2&filter-coursestatus-Active=on&filter-departmentcode-PE=on'),\n",
       " (aoerc pool hours,\n",
       "  'https://alumni.stanford.edu/get/page/membership/benefits/pools'),\n",
       " (aoerc pool hours, 'https://alumni.stanford.edu/get/page/perks/PoolAndGyms'),\n",
       " (aoerc pool hours, 'https://cardinalrec.stanford.edu/facilities/aoerc/'),\n",
       " (aoerc pool hours, 'https://events.stanford.edu/2014/February/24'),\n",
       " (aoerc pool hours, 'https://events.stanford.edu/2014/January/24'),\n",
       " (aoerc pool hours,\n",
       "  'https://explorecourses.stanford.edu/search;jsessionid=j22hm87qcrqtw3ryczmlq4kb?q=PE&view=catalog&filter-term-Winter=off&academicYear=&filter-term-Summer=off&filter-term-Autumn=off&filter-term-Spring=off&page=6&filter-coursestatus-Active=on&filter-departmentcode-PE=on'),\n",
       " (aoerc pool hours,\n",
       "  'https://med.stanford.edu/content/dam/sm/hip/documents/FitnessSchedule.pdf'),\n",
       " (aoerc pool hours,\n",
       "  'https://web.stanford.edu/dept/pe/cgi-bin/downloads/publications/20132014AnnualReport.pdf'),\n",
       " (swimming pool hours,\n",
       "  'http://news.stanford.edu/2016/03/22/human-tears-insight-032216/'),\n",
       " (swimming pool hours,\n",
       "  'https://aac2014.stanford.edu/sites/default/files/pdf-files/2013_info_and_amenity_sheet_with_map.pdf'),\n",
       " (swimming pool hours,\n",
       "  'https://energyclub.stanford.edu/the-untold-story-of-the-frac-hand/'),\n",
       " (swimming pool hours,\n",
       "  'https://med.stanford.edu/content/dam/sm/scalpel/documents/WellBeing/After%20Hours.2015.pdf'),\n",
       " (swimming pool hours,\n",
       "  'https://stanfordwest.stanford.edu/current-residents/amenities/fitness/pools-spa'),\n",
       " (swimming pool hours,\n",
       "  'https://stanfordwest.stanford.edu/current-residents/amenities/fitness/swim-lessons'),\n",
       " (swimming pool hours,\n",
       "  'https://web.stanford.edu/class/cs344a/papers/p154-vasilescu.pdf'),\n",
       " (swimming pool hours,\n",
       "  'https://web.stanford.edu/dept/scra/swim-generalinfo.html'),\n",
       " (swimming pool hours, 'https://web.stanford.edu/dept/scra/swim-rules.html'),\n",
       " (swimming pool hours,\n",
       "  'https://web.stanford.edu/group/parasites/ParaSites2009/NevinsANDLiu_Giardiasis/NevinsANDLiu_Giardiasis.htm'),\n",
       " (computer science, 'http://cs.stanford.edu/degrees/undergrad/'),\n",
       " (computer science, 'http://csmajor.stanford.edu/Considering.shtml'),\n",
       " (computer science,\n",
       "  'http://exploredegrees.stanford.edu/schoolofengineering/computerscience/'),\n",
       " (computer science, 'https://www-cs.stanford.edu/'),\n",
       " (computer science, 'https://www-cs.stanford.edu/about/contact-us'),\n",
       " (computer science, 'https://www-cs.stanford.edu/about/department-overview'),\n",
       " (computer science, 'https://www-cs.stanford.edu/academics/commencement'),\n",
       " (computer science, 'https://www-cs.stanford.edu/admissions/contact-us'),\n",
       " (computer science,\n",
       "  'https://www-cs.stanford.edu/admissions/current-stanford-students/coterminal-program'),\n",
       " (computer science,\n",
       "  'https://www-cs.stanford.edu/admissions/general-information'),\n",
       " (commencement schedule, 'https://commencement.stanford.edu/events/diploma'),\n",
       " (commencement schedule,\n",
       "  'https://commencement.stanford.edu/events/diploma/education'),\n",
       " (commencement schedule,\n",
       "  'https://commencement.stanford.edu/events/individual-diploma-ceremonies'),\n",
       " (commencement schedule,\n",
       "  'https://commencement.stanford.edu/events/nuestra-graduaci%C3%B3n-celebration-and-awards-ceremony'),\n",
       " (commencement schedule, 'https://commencement.stanford.edu/events/schedule'),\n",
       " (commencement schedule,\n",
       "  'https://commencement.stanford.edu/events/undergraduate-medals-ceremony'),\n",
       " (commencement schedule,\n",
       "  'https://commencement.stanford.edu/system/files/documents/ebrochure2016final.pdf'),\n",
       " (commencement schedule, 'https://ee.stanford.edu/academics/commencement'),\n",
       " (commencement schedule,\n",
       "  'https://registrar.stanford.edu/resources-and-help/stanford-academic-calendar-2016-17/2016-17-stanford-academic-calendar'),\n",
       " (parking permit, 'http://visit.stanford.edu/plan/parking.html'),\n",
       " (parking permit, 'https://transportation.stanford.edu/application-forms'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/commute-club/join-the-commute-club/2016-2017-permit-return-promotion'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/about-parking-permits/persons-disabilities'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/deal-with-parking-tickets/parking-enforcement'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/find-event-parking/buy-a-one-day-scratcher-permit'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/purchase-a-parking-permit/commuters'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/purchase-a-parking-permit/how-to-purchase'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/purchase-a-parking-permit/permit-prices-2016-17'),\n",
       " (parking permit,\n",
       "  'https://transportation.stanford.edu/parking/purchase-a-parking-permit/resident-students'),\n",
       " (stanford bookstore, 'http://events.stanford.edu/events/543/54377/'),\n",
       " (stanford bookstore, 'http://web.stanford.edu/group/bookstore/'),\n",
       " (stanford bookstore,\n",
       "  'http://web.stanford.edu/group/bookstore/SUprices/macintoshall.html'),\n",
       " (stanford bookstore,\n",
       "  'http://web.stanford.edu/group/bookstore/slaccontact.html'),\n",
       " (stanford bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/applecare.html'),\n",
       " (stanford bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/displaysMonitors.html'),\n",
       " (stanford bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/softwarecart.html'),\n",
       " (stanford bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/specials.html'),\n",
       " (stanford bookstore, 'https://web.stanford.edu/group/bookstore/contact.html'),\n",
       " (marguerite schedule,\n",
       "  'https://porterdrivecampus.stanford.edu/neighborhood/transportation-porter-drive'),\n",
       " (marguerite schedule, 'https://transportation.stanford.edu/marguerite'),\n",
       " (marguerite schedule, 'https://transportation.stanford.edu/marguerite/rp'),\n",
       " (marguerite schedule, 'https://transportation.stanford.edu/marguerite/slac'),\n",
       " (marguerite schedule, 'https://transportation.stanford.edu/marguerite/va'),\n",
       " (marguerite schedule,\n",
       "  'https://transportation.stanford.edu/marguerite/view-maps-and-schedules'),\n",
       " (marguerite schedule,\n",
       "  'https://transportation.stanford.edu/marguerite/view-maps-and-schedules/limited-holiday-marguerite-shuttle-service'),\n",
       " (marguerite schedule,\n",
       "  'https://transportation.stanford.edu/marguerite/view-maps-and-schedules/obtain-printed-map-and-schedule'),\n",
       " (marguerite schedule, 'https://transportation.stanford.edu/marguerite/y'),\n",
       " (marguerite schedule,\n",
       "  'https://transportation.stanford.edu/transit/express-bus/dumbarton-express'),\n",
       " (facility hours, 'http://library.stanford.edu/hours'),\n",
       " (facility hours, 'http://outdoored.stanford.edu/center/'),\n",
       " (facility hours, 'http://outdoored.stanford.edu/center/rentals/'),\n",
       " (facility hours,\n",
       "  'http://www-ssrl.slac.stanford.edu/content/contact-us/gate-hours-services'),\n",
       " (facility hours, 'https://alumni.stanford.edu/get/page/perks/PoolAndGyms'),\n",
       " (facility hours, 'https://biox.stanford.edu/about/building-services'),\n",
       " (facility hours, 'https://biox.stanford.edu/bio-x-poster-printer'),\n",
       " (facility hours,\n",
       "  'https://cardinalrec.stanford.edu/facilities/facility-hours/'),\n",
       " (facility hours,\n",
       "  'https://ehs.stanford.edu/about-us/occupational-health-center'),\n",
       " (dining hall hours,\n",
       "  'https://rde.stanford.edu/dining/caf%C3%A9%E2%80%99s-and-market-locations-and-hours'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/dining-hall-hours'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/late-night-dining'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/ricker-dining'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/stern-dining'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/summer-2016'),\n",
       " (dining hall hours,\n",
       "  'https://rde.stanford.edu/dining/sustainable-food-program'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/thanksgiving-hours'),\n",
       " (dining hall hours, 'https://rde.stanford.edu/dining/visitors-gateway'),\n",
       " (stanford visitor parking, 'http://visit.stanford.edu/'),\n",
       " (stanford visitor parking, 'http://visit.stanford.edu/plan/parking.html'),\n",
       " (stanford visitor parking,\n",
       "  'https://dschool.stanford.edu/groups/dhandbook/wiki/c68f5/Visitors.html'),\n",
       " (stanford visitor parking,\n",
       "  'https://museum.stanford.edu/visit/visit_parking.html'),\n",
       " (stanford visitor parking, 'https://naismet2016.stanford.edu/directions'),\n",
       " (stanford visitor parking, 'https://transportation.stanford.edu/parking'),\n",
       " (stanford visitor parking,\n",
       "  'https://transportation.stanford.edu/parking/deal-with-parking-tickets/parking-enforcement'),\n",
       " (stanford visitor parking,\n",
       "  'https://transportation.stanford.edu/parking/purchase-a-parking-permit/visitors'),\n",
       " (stanford visitor parking,\n",
       "  'https://transportation.stanford.edu/visitor-parking-rates'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/coming-stanford/new-student-information/international-student-orientation'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/immigration/employment'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/immigration/employment/f-1-employment/optional-practical-training-opt'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/immigration/maintaining-status'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/practical-matters'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/practical-matters/social-security-number'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/programs-and-events/events-center'),\n",
       " (bechtel international center,\n",
       "  'https://bechtel.stanford.edu/programs-and-events/spouses-partners-and-families'),\n",
       " (lakeside dining hours,\n",
       "  'http://web.stanford.edu/dept/rde/dining/facebooklanding.html'),\n",
       " (lakeside dining hours,\n",
       "  'https://rde.stanford.edu/dining/arrillaga-family-dining-commons'),\n",
       " (lakeside dining hours,\n",
       "  'https://rde.stanford.edu/dining/chef-inspired-seasonal-kitchen-table'),\n",
       " (lakeside dining hours, 'https://rde.stanford.edu/dining/dining-hall-hours'),\n",
       " (lakeside dining hours, 'https://rde.stanford.edu/dining/events-calendar'),\n",
       " (lakeside dining hours, 'https://rde.stanford.edu/dining/lakeside-dining'),\n",
       " (lakeside dining hours,\n",
       "  'https://rde.stanford.edu/dining/meal-plan-frequently-asked-questions'),\n",
       " (lakeside dining hours, 'https://rde.stanford.edu/dining/midnight-breakfast'),\n",
       " (lakeside dining hours,\n",
       "  'https://web.stanford.edu/dept/rde/dining/pdfs/RFRA_2010Guidebook.pdf'),\n",
       " (bookstore,\n",
       "  'http://web.stanford.edu/group/bookstore/SUprices/macintoshall.html'),\n",
       " (bookstore, 'http://web.stanford.edu/group/bookstore/taxfree.html'),\n",
       " (bookstore,\n",
       "  'https://arts.stanford.edu/event/stanford-bookstore-meet-the-author-jacob-towery/'),\n",
       " (bookstore,\n",
       "  'https://cardinalatwork.stanford.edu/benefits-rewards/sweeteners/stanford-bookstore-discounts'),\n",
       " (bookstore, 'https://events.stanford.edu/byOrganization/158/'),\n",
       " (bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/a_acclist.html'),\n",
       " (bookstore,\n",
       "  'https://web.stanford.edu/group/bookstore/SUprices/specials.html'),\n",
       " (green library hours, 'http://library.stanford.edu/'),\n",
       " (green library hours, 'http://library.stanford.edu/hours'),\n",
       " (green library hours, 'http://library.stanford.edu/libraries/branner/about'),\n",
       " (green library hours, 'http://library.stanford.edu/libraries/green/about'),\n",
       " (green library hours, 'http://library.stanford.edu/libraries/music/about'),\n",
       " (green library hours,\n",
       "  'http://library.stanford.edu/libraries/philosophy/about'),\n",
       " (green library hours, 'http://library.stanford.edu/spc'),\n",
       " (green library hours, 'https://library.stanford.edu/green/study-rooms'),\n",
       " (green library hours,\n",
       "  'https://library.stanford.edu/libraries/green/green-library-group-study-rooms'),\n",
       " (green library hours,\n",
       "  'https://library.stanford.edu/libraries/lathrop/24-hour-study-room'),\n",
       " (memorial church, 'http://125.stanford.edu/then-and-now/memorial-church/'),\n",
       " (memorial church,\n",
       "  'http://web.stanford.edu/group/religiouslife/cgi-bin/wordpress/memorial-church/history/'),\n",
       " (memorial church,\n",
       "  'http://web.stanford.edu/group/religiouslife/cgi-bin/wordpress/memorial-church/holiday-services/'),\n",
       " (memorial church,\n",
       "  'http://web.stanford.edu/group/religiouslife/cgi-bin/wordpress/memorial-church/memorial-services/'),\n",
       " (memorial church,\n",
       "  'http://web.stanford.edu/group/religiouslife/cgi-bin/wordpress/memorial-church/weddings/wedding-faqs/'),\n",
       " (memorial church, 'https://events.stanford.edu/events/617/61741'),\n",
       " (memorial church,\n",
       "  'https://live.stanford.edu/plan-your-visit/venues/memorial-church'),\n",
       " (memorial church,\n",
       "  'https://music.stanford.edu/performing-group/memorial-church-choir'),\n",
       " (memorial church,\n",
       "  'https://music.stanford.edu/venues/memorial-church/memorial-church-accessibility'),\n",
       " (memorial church, 'https://religiouslife.stanford.edu/')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the title/headers to ids instead of words\n",
    "def convert_to_ids(list_of_words):\n",
    "    return [word_map.get_id(word) for word in list_of_words]\n",
    "\n",
    "for query, url_dict in query_dict.items():\n",
    "    for url, document in url_dict.items():\n",
    "        if document.title:\n",
    "            document.title = convert_to_ids(document.title)\n",
    "        \n",
    "        if document.headers:\n",
    "            document.headers = [convert_to_ids(header) for header in document.headers]\n",
    "            \n",
    "        doc_id = doc_dict[url]\n",
    "        if doc_id in docId_to_content:\n",
    "            _, document.body_content = docId_to_content[doc_id]\n",
    "        else:\n",
    "            document.body_content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://events.stanford.edu/2014/February/18/': 0.0, 'http://events.stanford.edu/2014/February/6/': 0.0, 'http://events.stanford.edu/2014/March/13/': 0.0, 'http://events.stanford.edu/2014/March/3/': 0.0, 'http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf': 0.0, 'http://web.stanford.edu/group/masters/pool.html': 1.0, 'https://alumni.stanford.edu/get/page/perks/PoolAndGyms': 1.5, 'https://cardinalrec.stanford.edu/facilities/aoerc/': 2.0, 'https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=': 0.5, 'https://glo.stanford.edu/events/stanford-rec-open-house': 0.5}\n",
      "\n",
      "{'http://alumni.stanford.edu/get/page/membership/benefits/creditcard': 2.0, 'http://alumni.stanford.edu/get/page/membership/benefits/libraries': 2.0, 'http://alumni.stanford.edu/get/page/membership/students': 2.0, 'https://alumni-esc.stanford.edu/get/page/membership/faq-general': 2.0, 'https://alumni.stanford.edu/get/page/landing/resources': 2.0, 'https://alumni.stanford.edu/get/page/membership/benefits': 2.0, 'https://alumni.stanford.edu/get/page/membership/benefits/golf': 2.0, 'https://alumni.stanford.edu/get/page/membership/benefits/rh_alums': 2.0, 'https://alumni.stanford.edu/get/page/membership/join?memb_group=SAA': 2.0, 'https://alumni.stanford.edu/get/page/perks/index': 2.0}\n",
      "\n",
      "{'https://alcohol.stanford.edu/cardinal-nights': 3.0, 'https://alcohol.stanford.edu/cardinal-nights/about-cardinal-nights': 2.5, 'https://alcohol.stanford.edu/events/cardinal-nights-build-critter': 2.0, 'https://alcohol.stanford.edu/events/cardinal-nights-chocolate-tasting': 2.0, 'https://alcohol.stanford.edu/events/cardinal-nights-fall-carnival': 2.0, 'https://alcohol.stanford.edu/events/cardinal-nights-homecoming-dance': 2.0, 'https://alcohol.stanford.edu/events/cardinal-nights-mini-grant-space-jam-lion-king-screening': 2.0, 'https://alcohol.stanford.edu/events/cardinal-nights-presents-book-mormon': 2.0, 'https://news.stanford.edu/2018/10/11/expanding-weekend-finding-community-cardinal-nights/': 2.0, 'https://parents.stanford.edu/2017/11/17/having-fun-at-stanford-without-alcohol/': 1.5}\n",
      "\n",
      "{'http://ee380.stanford.edu/': 0.0, 'http://forum.stanford.edu/': 3.0, 'http://forum.stanford.edu/about/mailinglist.php': 2.0, 'http://forum.stanford.edu/events/annualmeeting.php': 2.0, 'http://forum.stanford.edu/events/calendar.php': 2.0, 'https://forum.stanford.edu/events/careerfair_affiliatefactsheet2016.php': 2.0, 'https://forum.stanford.edu/research/areaprofile.php?areaid=5': 2.0, 'https://forum.stanford.edu/research/projects.php': 2.0, 'https://forum.stanford.edu/visitors/directions/huang.php': 1.5, 'https://www-cs.stanford.edu/forum': 2.5}\n",
      "\n",
      "{'http://liblog.law.stanford.edu/computer-labs-in-the-law-library-faq/': 2.0, 'http://library.stanford.edu/hours': 1.0, 'https://law.stanford.edu/robert-crown-law-library/': 3.0, 'https://law.stanford.edu/robert-crown-law-library/about-the-law-library/': 3.0, 'https://law.stanford.edu/robert-crown-law-library/hours/': 2.0, 'https://law.stanford.edu/robert-crown-law-library/law-library-conference-rooms/': 2.0, 'https://law.stanford.edu/robert-crown-law-library/library-services/printing-photocopying-and-scanning/': 2.0, 'https://law.stanford.edu/robert-crown-law-library/library-staff/': 2.0, 'https://law.stanford.edu/robert-crown-law-library/research-resources/': 2.0, 'https://law.stanford.edu/robert-crown-law-library/research-resources/empirical-legal-research/': 2.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a few example relevances\n",
    "num_dicts = 0\n",
    "for query in query_dict:\n",
    "    print(get_relevance_dict(query_dict, query))\n",
    "    print()\n",
    "    num_dicts += 1\n",
    "    if num_dicts == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''having generated query content and doc content, lets try ranking by cosine similarity between query and document \n",
    "embedding\n",
    "'''\n",
    "#iteration 1: ignore words not there in the embedding\n",
    "\n",
    "#lookup function\n",
    "GLOVE_HOME = os.path.join('data', 'glove.6B')\n",
    "glove_lookup = utils.glove2dict(os.path.join(GLOVE_HOME, 'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_glove_embedding(words):\n",
    "    for word in words:\n",
    "        assert isinstance(word, str), (type(word), word)\n",
    "    allvecs = np.array([glove_lookup[w] for w in words if w in glove_lookup]) \n",
    "\n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(glove_lookup.values())))\n",
    "        feats = np.zeros(dim)    \n",
    "    else:       \n",
    "        feats = np.mean(allvecs, axis=0) \n",
    "    \n",
    "    return feats\n",
    "\n",
    "def query_and_document_embeddings(query, doc_content_type='title'):\n",
    "    query_embedding = make_glove_embedding(query)\n",
    "    \n",
    "    document_entries = query_dict[query]\n",
    "    \n",
    "    def get_doc_words(document):\n",
    "        title_words = [word_map.get_string(word_id) for word_id in document.title]\n",
    "        \n",
    "        if doc_content_type == 'title':\n",
    "            return title_words\n",
    "        \n",
    "        elif doc_content_type == 'headers':\n",
    "            if not document.headers:\n",
    "                return title_words\n",
    "            words = []\n",
    "            for header in document.headers:\n",
    "                words += [word_map.get_string(word_id) for word_id in header]\n",
    "            return words\n",
    "        \n",
    "        if doc_content_type == 'th': # use title and header combo\n",
    "            if not document.headers:\n",
    "                return title_words\n",
    "            words = 2 * title_words\n",
    "            for header in document.headers:\n",
    "                words += [word_map.get_string(word_id) for word_id in header]\n",
    "            return words\n",
    "        \n",
    "        elif doc_content_type == 'body_hits':\n",
    "            if not document.body_hits:\n",
    "                return title_words\n",
    "            \n",
    "            words = []\n",
    "            for query_word, hits in document.body_hits.items():\n",
    "                words += len(hits) * [query_word]\n",
    "            return words\n",
    "        \n",
    "        elif doc_content_type == 'body_content':\n",
    "            if not document.body_content:\n",
    "                return title_words\n",
    "            return [word_map.get_string(word_id) for word_id in document.body_content]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid doc content: {}\".format(doc_content_type))\n",
    "    \n",
    "    document_tuples = [(url, get_doc_words(document)) for url, document in document_entries.items()]\n",
    "    document_embeddings = [(url, make_glove_embedding(words)) for url, words in document_tuples]\n",
    "    return query_embedding, document_embeddings\n",
    "\n",
    "def cosine_similarity(doc_embedding, query_embedding):\n",
    "    norm = np.sqrt(np.sum(np.square(doc_embedding))) # normalize the documents, but not the queries\n",
    "    if norm > 0:\n",
    "        doc_embedding /= norm\n",
    "    return np.dot(doc_embedding, query_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG(ranked_docs, relevance_dict):\n",
    "    '''This function takes an ordered/ranked document list with the ground truth relevance labels from the \n",
    "    relevance_dict and returns a DCG score for the retrieval/ranking.\n",
    "    Input -- \n",
    "        ranked_docs = list of doc IDs ordered by rank. First element in the list is the highest ranked\n",
    "        relevance_dict = dict with keys as the doc_IDs and relevance score as the element\n",
    "    Output -- \n",
    "        DCG [float]'''\n",
    "    \n",
    "    discount_factor = []\n",
    "    relevance_scores = []\n",
    "    for i, doc in enumerate(ranked_docs):\n",
    "        rank = i+1\n",
    "        if rank == 1:\n",
    "            discount_factor.append(1)\n",
    "        else:\n",
    "            discount_factor.append(log2(rank))\n",
    "        relevance_scores.append(relevance_dict[doc])\n",
    "    \n",
    "    return sum(np.array(relevance_scores)/np.array(discount_factor))\n",
    "            \n",
    "def NDCG(ranked_docs, relevance_dict):\n",
    "    '''This function takes an ordered/ranked document list with the ground truth relevance labels from the \n",
    "    relevance_dict and returns a NDCG score for the ranking. \n",
    "    Input -- \n",
    "        ranked_docs = list of doc IDs ordered by rank. First element in the list is the highest ranked\n",
    "        relevance_dict = dict with keys as the doc_IDs and relevance score as the element\n",
    "    Output -- \n",
    "        NDCG [float]'''\n",
    "    ideal_ordering, _ = zip(*sorted(relevance_dict.items(), key = lambda x: (-x[1])))\n",
    "    ideal_ordering = list(ideal_ordering)\n",
    "    DCG_oracle = DCG(ideal_ordering, relevance_dict)\n",
    "    DCG_case = DCG(ranked_docs, relevance_dict)\n",
    "    \n",
    "    #return 0 if DCG_ideal is 0 (happens when all the retrieved docs are rated 0)\n",
    "    if DCG_oracle == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return DCG_case/DCG_oracle\n",
    "\n",
    "#sanity check\n",
    "ranked_docs = [0,1,2,3,4]\n",
    "relevance_dict = {0: 2, 1: 3, 2: 0, 3: 0, 4: 1}\n",
    "DCG_score = 2 + 3/log2(2) + 1/log2(5)\n",
    "Ideal_score = 3 + 2/log2(2) + 1/log2(3)\n",
    "NDCG_score = DCG_score/Ideal_score\n",
    "assert DCG(ranked_docs, relevance_dict) == DCG_score, \"DCG error\"\n",
    "assert NDCG(ranked_docs, relevance_dict) == NDCG_score, \"NDCG error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_helper(relevance_list):\n",
    "    precision = 0.0\n",
    "    relevant_so_far = 0.0\n",
    "    for i, val in enumerate(relevance_list):\n",
    "        relevant_so_far += val\n",
    "        precision += relevant_so_far / (i+1)\n",
    "    return precision / len(relevance_list)\n",
    "\n",
    "def average_precision(ranked_doc_list, query_relevance_dict):\n",
    "    relevance_list = [1 if query_relevance_dict[doc] >= 1.0 else 0 for doc in ranked_doc_list]\n",
    "    return average_precision_helper(relevance_list)\n",
    "\n",
    "expected = (1 + 1 + 2/3 + 2/4 + 3/5 + 3/6 + 4/7)/ 7\n",
    "actual = average_precision_helper([1, 1, 0, 0, 1, 0, 1])\n",
    "assert expected == actual, actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics(content_type='title', scoring_func=cosine_similarity):\n",
    "\n",
    "    ndcg_sum = 0.0\n",
    "    precision_sum = 0.0\n",
    "    for query in query_dict:\n",
    "        query_relevance_dict = get_relevance_dict(query_dict, query)\n",
    "        \n",
    "        query_embedding, document_embeddings = query_and_document_embeddings(query, content_type)\n",
    "        \n",
    "        scores = [(url, scoring_func(doc_emb, query_embedding)) for url, doc_emb in document_embeddings]\n",
    "        scores = sorted(scores, key = lambda x: x[1], reverse=True)\n",
    "        ranked_doc_list, _ = zip(*scores)\n",
    "        ranked_doc_list = list(ranked_doc_list)\n",
    "\n",
    "        ndcg_sum += NDCG(ranked_doc_list, query_relevance_dict)\n",
    "        precision_sum += average_precision(ranked_doc_list, query_relevance_dict)\n",
    "        \n",
    "    ndcg_sum = ndcg_sum / len(query_dict)\n",
    "    precision_sum = precision_sum / len(query_dict)\n",
    "    return {'NDCG': ndcg_sum,\n",
    "            'MAP': precision_sum}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_similarity(doc_embedding, query_embedding):\n",
    "    return np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.80535469840465, 'MAP': 0.7341629020255825}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(scoring_func=random_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.7998466704959435, 'MAP': 0.7260548526042633}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(content_type='headers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8058287842182414, 'MAP': 0.7282281456941095}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(content_type='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8033349213913299, 'MAP': 0.7292918295655904}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(content_type='th') # use combo of title and headers as in old code version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.8064515680022327, 'MAP': 0.7278642805097923}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(content_type='body_hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NDCG': 0.842848133595507, 'MAP': 0.7634415501773494}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_metrics(content_type='body_content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BSBI.dict gives the:\n",
    "posting_dict - \n",
    "        [dict mapping termID to (start position in the index file, \n",
    "                                number of postings in the list, \n",
    "                                length in bytes of posting list)] \n",
    "and termID - [list]\n",
    "'''\n",
    "with open(os.path.join(data_dir, \"BSBI.dict\"), \"rb\") as f:\n",
    "    posting_dict, termsID = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing signal and relevance train files...\n",
      "\n",
      "Signal File\n",
      "749\n",
      "749\n",
      "749\n",
      "\n",
      "--------------------\n",
      "\n",
      "\n",
      "Relevance File\n",
      "749\n",
      "749\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing signal and relevance train files...\")\n",
    "\n",
    "print(\"\\nSignal File\")\n",
    "query_dict = {} #maps queries to query id (Assuming distinct queries)\n",
    "doc_dict = {}  #maps docs to doc id\n",
    "query_doc_dict = {} #maps query ids to list of doc ids\n",
    "\n",
    "query_id_list = [] #list\n",
    "doc_id_list = []\n",
    "\n",
    "doc_list_for_query = []\n",
    "\n",
    "query_repetitions = {} #dict mapping queries to number of repetitions\n",
    "query_counter = 0\n",
    "doc_repetitions = 0\n",
    "\n",
    "with open(os.path.join(data_dir, \"pa3.signal.train\"), \"r\", encoding='utf8') as f:\n",
    "    last_query_id = 0 \n",
    "    for line in f:\n",
    "        line_list = line.split()\n",
    "        if line_list[0] == 'query:':\n",
    "            query_counter += 1\n",
    "            if query_counter >= 2:\n",
    "                query_doc_dict[last_query_id] = doc_list_for_query\n",
    "                \n",
    "            query = \" \".join(line_list[1:])\n",
    "            \n",
    "            if query_dict.get(query, None) != None:\n",
    "                query_repetitions[query] = query_repetitions.get(query, 0) + 1\n",
    "                query = query + \" _\" + str(query_repetitions[query])\n",
    "            \n",
    "            query_id_list.append(query)\n",
    "            query_dict[query] = len(query_id_list) - 1\n",
    "            \n",
    "            #query_doc_dict[last_query_id] = doc_list_for_query\n",
    "            last_query_id = len(query_id_list) - 1 #update the last query whenever a new query starts\n",
    "            doc_list_for_query = [] #reinitialize the doc list whenever a new query starts\n",
    "        \n",
    "        elif line_list[0] == 'url:':\n",
    "            assert len(line_list) == 2, \"line_list for url has more than 2 entries. Please check!\"\n",
    "            doc = line_list[1]\n",
    "            if doc_dict.get(doc, None) == None:\n",
    "                doc_id_list.append(doc)\n",
    "                doc_id = len(doc_id_list) -1\n",
    "                doc_dict[doc] = doc_id\n",
    "            else:\n",
    "                doc_id = doc_dict[doc]\n",
    "                \n",
    "            doc_list_for_query.append(doc_id)\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    query_doc_dict[last_query_id] = doc_list_for_query\n",
    "            \n",
    "print(query_counter)\n",
    "print(len(query_dict))\n",
    "print(len(query_doc_dict))\n",
    "    \n",
    "print(\"\\n\" + \"--\"*10 + \"\\n\")\n",
    "\n",
    "import copy\n",
    "query_total_repetitions = copy.deepcopy(query_repetitions)\n",
    "query_doc_relevance = {}\n",
    "doc_relevance_dict = {}\n",
    "query_counter = 0\n",
    "print(\"\\nRelevance File\")\n",
    "with open(os.path.join(data_dir, \"pa3.rel.train\"), \"r\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line_list = line.split()\n",
    "        if line_list[0] == 'query:':\n",
    "            query_counter += 1\n",
    "            query = \" \".join(line_list[1:])\n",
    "            if query_repetitions.get(query, None) != None:\n",
    "                query_repetition_number = query_total_repetitions[query] - query_repetitions[query]\n",
    "                query_repetitions[query] -= 1\n",
    "                if query_repetition_number != 0:\n",
    "                    query = query + \" _\" + str(query_repetition_number)\n",
    "            \n",
    "            if query_counter >= 2:\n",
    "                assert query_doc_relevance.get(last_query_id, None) == None, \"Query already existed in the relevance dict\"\n",
    "                query_doc_relevance[last_query_id] = doc_relevance_dict\n",
    "            \n",
    "            last_query_id = query_dict[query]\n",
    "            doc_relevance_dict = {}\n",
    "            \n",
    "        elif line_list[0] == \"url:\":\n",
    "            doc = line_list[1]\n",
    "            docID = doc_dict[doc]\n",
    "            \n",
    "            doc_relevance_dict[docID] = float(line_list[-1].strip())\n",
    "    \n",
    "    query_doc_relevance[last_query_id] = doc_relevance_dict\n",
    "\n",
    "print(query_counter)\n",
    "print(len(query_doc_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(doc_dict, open(\"doc_dict.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples\n",
      "\n",
      "..........\n",
      "Sample Query:  stanford aoerc pool hours\n",
      "\n",
      "----------\n",
      "Sample Query Id:  0\n",
      "\n",
      "----------\n",
      "Query Repetitions {'facility hours': 1, 'stanford dining hours': 1, 'stanford bookstore': 1, 'aoerc pool hours': 1, 'arrillaga gym hours': 1, 'lakeside dining hours': 1, 'marguerite schedule': 1, 'computer science': 1, 'commencement schedule': 1, 'parking permit': 1, 'bookstore': 1, 'green library hours': 1, 'memorial church': 1, 'dining hall hours': 1, 'cardinal nights': 1, 'swimming pool hours': 1, 'bechtel international center': 1, 'stanford visitor parking': 1}\n",
      "\n",
      "----------\n",
      "Manipulated query repetitions:  {'facility hours': -1, 'stanford dining hours': -1, 'stanford bookstore': -1, 'aoerc pool hours': -1, 'arrillaga gym hours': -1, 'lakeside dining hours': -1, 'marguerite schedule': -1, 'computer science': -1, 'commencement schedule': -1, 'parking permit': -1, 'bookstore': -1, 'green library hours': -1, 'memorial church': -1, 'dining hall hours': -1, 'cardinal nights': -1, 'swimming pool hours': -1, 'bechtel international center': -1, 'stanford visitor parking': -1}\n",
      "\n",
      "----------\n",
      "Retrieved doc Ids:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "----------\n",
      "Retrieved doc relevance:  {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 1.0, 6: 1.5, 7: 2.0, 8: 0.5, 9: 0.5}\n",
      "\n",
      "----------\n",
      "Retrieved docs:  ['http://events.stanford.edu/2014/February/18/', 'http://events.stanford.edu/2014/February/6/', 'http://events.stanford.edu/2014/March/13/', 'http://events.stanford.edu/2014/March/3/', 'http://med.stanford.edu/content/dam/sm/hip/documents/FreeFitnessWeek.pdf', 'http://web.stanford.edu/group/masters/pool.html', 'https://alumni.stanford.edu/get/page/perks/PoolAndGyms', 'https://cardinalrec.stanford.edu/facilities/aoerc/', 'https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=PE+128%3A+Swimming%3A+Beginning+I&collapse=', 'https://glo.stanford.edu/events/stanford-rec-open-house']\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sample_query = query_id_list[0]\n",
    "sample_query_id = query_dict[sample_query]\n",
    "retrieved_docIds = query_doc_dict[sample_query_id]\n",
    "retrieved_relevance = query_doc_relevance[sample_query_id]\n",
    "retrieved_docs = [doc_id_list[docId] for docId in retrieved_docIds]\n",
    "\n",
    "print(\"Samples\", end = \"\\n\\n\" + \".\"*10 + \"\\n\")\n",
    "print(\"Sample Query: \", sample_query, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Sample Query Id: \", sample_query_id, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Query Repetitions\", query_total_repetitions, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Manipulated query repetitions: \", query_repetitions, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Retrieved doc Ids: \", retrieved_docIds, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Retrieved doc relevance: \", retrieved_relevance, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")\n",
    "print(\"Retrieved docs: \", retrieved_docs, end = \"\\n\\n\" + \"-\"*10 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Analysis\n",
      "\n",
      "Total queries:  749\n",
      "Number of unique queries:  731\n",
      "% of unique queries:  97.59679572763686\n",
      "\n",
      "----------\n",
      "\n",
      "Retrieved doc Analysis\n",
      "\n",
      "Number of unique docs:  5256\n",
      "Docs per query:  10\n",
      "% of unique docs:  70.173564753004\n",
      "\n",
      "----------\n",
      "\n",
      "Label Analysis\n",
      "\n",
      "Set of labels:  {0.0, 1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 2.3, 1.7, 1.3, 2.7, 2.2, 2.8, 0.3, 0.7, 1.8}\n"
     ]
    }
   ],
   "source": [
    "#number of unique queries\n",
    "n_queries = len(query_dict)\n",
    "n_unique_queries = len(query_dict)\n",
    "for repeated_query in query_total_repetitions:\n",
    "    n_unique_queries -= query_total_repetitions[repeated_query]\n",
    "\n",
    "\n",
    "print(\"Query Analysis\\n\")\n",
    "print(\"Total queries: \", n_queries)\n",
    "print(\"Number of unique queries: \", n_unique_queries)\n",
    "print(\"% of unique queries: \", (n_unique_queries/n_queries)*100)\n",
    "print(\"\\n\" + \"-\"*10 + \"\\n\")\n",
    "\n",
    "print(\"Retrieved doc Analysis\\n\")\n",
    "total_docs = len(doc_list_for_query) * n_queries\n",
    "print(\"Number of unique docs: \", len(doc_id_list))\n",
    "print(\"Docs per query: \", len(doc_list_for_query))\n",
    "print(\"% of unique docs: \", (len(doc_id_list)/(len(doc_list_for_query) * n_queries))*100)\n",
    "print(\"\\n\" + \"-\"*10 + \"\\n\")\n",
    "\n",
    "print(\"Label Analysis\\n\")\n",
    "label_set = []\n",
    "for _, relavance_dict in query_doc_relevance.items():\n",
    "    _, labels = zip(*tuple(relavance_dict.items()))\n",
    "    labels = list(labels)\n",
    "    label_set += labels\n",
    "label_set = set(label_set)\n",
    "print(\"Set of labels: \", label_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric - NDCG \n",
    "\n",
    "can also incorporate Precision, MAP, etc. after binary conversion with decay rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random ordering accuracy\n",
    "\n",
    "For every query, arrange the docs in random order and check the NDCG value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594.1691182892581\n",
      "749\n",
      "0.7932832019883286\n"
     ]
    }
   ],
   "source": [
    "#randomly shuffles the docs\n",
    "ndcg_sum = 0.0\n",
    "for queryID, doc_list in query_doc_dict.items():\n",
    "    np.random.shuffle(doc_list)\n",
    "    ndcg_sum += NDCG(doc_list, query_doc_relevance[queryID])\n",
    "print(ndcg_sum)\n",
    "print(len(query_id_list))\n",
    "print(ndcg_sum/len(query_id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing query and document embeddings\n",
    "\n",
    "Document embeddings are obtained from the given title or header information without any weight normalization. Loop through the files and collect doc words by looking in the title and header (one idea can be to give more weight to title than to header). Lookup for each word in the glove embedding. Choose and fix a random combination of word if a word in query does not exist (maybe a combination from the words university and around because the corpus relates to stanford). Ignore otherwise. Finally, find cosine similarity and rank and compute NDCG score. \n",
    "\n",
    "#### Other ideas:\n",
    "1. Treat upper case and start of line word different than end of line word, etc\n",
    "2. Can add word correction, etc\n",
    "3. How scraping documents and adding more words to document effect performance\n",
    "4. Modeling item-item dependency by seq2slate architecture\n",
    "5. Creating embedding for words in the query but not in the embedding vocab as a distinct combination for \n",
    "6. Training word2vec on this and then trying different ideas with the center and context matrices obtained\n",
    "7. DESM type ideas with the embeddings of words in the document weighted by the similarity of words (W_out * q_emb)\n",
    "8. Treating re-ranking task as an NLI task where document entails query\n",
    "9. regressing score for each query-doc pair using nlp inspired regression by predicting score through RNN for instance\n",
    "10. experimenting with listwise and pairwise approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now expand the entires in query dict to include the body text (encoded as integers, \n",
    "# lookup using vocab_id_list/vocab_dict)\n",
    "\n",
    "lacking_body = []\n",
    "for query, query_entry in query_dict.items():\n",
    "    for url, url_entry in query_entry.items():\n",
    "        doc_id = doc_dict[url]\n",
    "        if doc_id in docId_to_content:\n",
    "            _, body_content = docId_to_content[doc_id]\n",
    "            url_entry.body_content = body_content\n",
    "        else:\n",
    "            url_entry.body_content = None\n",
    "            lacking_body.append(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(lacking_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words 5390418\n",
      "Number of unique words:  92671\n",
      "% of uniqueness: 1.72 %\n"
     ]
    }
   ],
   "source": [
    "#checking vocab etc\n",
    "total_words = 0\n",
    "for repeated_word in vocab_frequency:\n",
    "    total_words += vocab_frequency[repeated_word]\n",
    "\n",
    "print(\"Total number of words\", total_words)\n",
    "print(\"Number of unique words: \", len(vocab_id_list))\n",
    "print(\"% of uniqueness: {:.2f} %\".format((len(vocab_id_list)/total_words)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "5\n",
      "\n",
      "--------------------\n",
      "\n",
      "Sample of query content and doc content\n",
      "Query content:\n",
      " [0, 1890, 2, 866]\n",
      "\n",
      "Doc sample:\n",
      " ([0, 207, 614, 615], [179, 51, 616, 180, 0, 58, 0, 207, 617, 386, 185, 379, 386, 237, 618, 619, 620, 338, 456, 572, 621, 384, 622, 623, 181, 624, 625, 626, 627, 264, 331, 628, 338, 629, 579, 514, 630, 631, 4, 632, 122, 633, 634, 635, 636, 637, 435, 638, 639, 640, 123, 641, 642, 58, 207, 338, 643, 272, 138, 644, 645, 646, 647, 648, 138, 649, 650, 138, 651, 652, 377, 653, 654, 655, 656, 657, 658, 659, 660, 365, 661, 662, 663, 338, 353, 386, 185, 379, 386, 237, 614, 615, 664, 665, 122, 633, 24, 1321, 288, 123, 777, 1322, 58, 76, 1323, 614, 669, 76, 24, 1324, 1325, 12, 632, 1326, 1327, 259, 1328, 185, 24, 90, 386, 614, 669, 1329, 1330, 324, 12, 632, 1326, 1327, 259, 1328, 185, 24, 90, 386, 614, 669, 1331, 31, 24, 1332, 507, 12, 632, 199, 51, 1333, 24, 1334, 614, 669, 711, 733, 734, 514, 507, 579, 31, 24, 667, 1335, 45, 192, 1336, 614, 669, 1004, 370, 1005, 671, 222, 672, 785, 734, 138, 1337, 408, 1338, 1339, 45, 646, 31, 24, 43, 614, 669, 1340, 776, 138, 1341, 1342, 185, 661, 1343, 1344, 1345, 614, 669, 1346, 1347, 680, 122, 633, 1348, 1349, 31, 1350, 1351, 45, 24, 1352, 76, 226, 1353, 614, 669, 462, 692, 507, 694, 695, 696, 122, 633, 952, 406, 256, 249, 406, 259, 256, 249, 998, 285, 333, 614, 669, 1082, 1005, 813, 122, 633, 1005, 1354, 29, 0, 614, 669, 1355, 1356, 677, 123, 235, 45, 259, 1357, 1358, 614, 669, 1359, 1360, 0, 138, 571, 1361, 146, 549, 122, 338, 1362, 1363, 45, 1364, 259, 1365, 31, 614, 669, 673, 507, 1366, 122, 633, 32, 1367, 259, 24, 745, 1368, 1369, 1370, 1371, 1372, 614, 669, 1172, 680, 507, 138, 259, 830, 29, 53, 226, 896, 279, 1373, 1374, 259, 1375, 614, 669, 0, 658, 1156, 680, 554, 138, 24, 823, 0, 824, 554, 19, 825, 826, 827, 828, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 147, 76, 830, 831, 708, 832, 29, 833, 554, 19, 834, 826, 835, 615, 613, 836, 837, 28, 838, 412, 742, 351, 839, 579, 0, 840, 841, 842, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 843, 28, 348, 437, 377, 435, 28, 843, 28, 122, 633, 844, 842, 212, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 843, 28, 579, 845, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 843, 28, 348, 437, 377, 435, 28, 843, 28, 579, 846, 847, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 843, 28, 348, 437, 377, 435, 28, 843, 28, 579, 848, 554, 19, 827, 615, 691, 615, 848, 849, 850, 38, 412, 851, 719, 852, 351, 839, 579, 1376, 308, 140, 185, 1377, 1378, 554, 19, 827, 615, 891, 615, 192, 1000, 579, 853, 361, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 579, 854, 855, 554, 19, 827, 615, 691, 615, 348, 856, 674, 579, 857, 853, 361, 554, 19, 827, 615, 691, 615, 26, 858, 2, 29, 24, 348, 437, 377, 435, 28, 579, 859, 860, 212, 554, 19, 827, 615, 691, 615, 348, 437, 377, 435, 28, 843, 28, 579, 861, 862, 554, 19, 827, 615, 691, 615, 348, 28, 185, 434, 435, 348, 437, 377, 435, 28, 579, 437, 377, 863, 830, 554, 19, 827, 615, 691, 615, 348, 437, 377, 435, 28, 579, 1379, 95, 1380, 554, 19, 1381, 826, 614, 867, 1382, 316, 258, 0, 1261, 259, 24, 348, 28, 185, 434, 259, 1383, 138, 695, 864, 30, 865, 866, 554, 19, 827, 615, 825, 867, 0, 695, 864, 138, 868, 98, 554, 19, 827, 615, 614, 867, 869, 870, 0, 708, 430, 871, 872, 873, 138, 874, 259, 875, 259, 876, 31, 100, 708, 554, 19, 834, 826, 691, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 877, 259, 500, 878, 554, 19, 879, 826, 614, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 24, 880, 881, 338, 882, 883, 554, 19, 834, 826, 330, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 24, 884, 259, 556, 29, 885, 554, 19, 834, 826, 330, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 886, 887, 259, 888, 889, 76, 24, 890, 554, 19, 825, 826, 891, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 892, 338, 893, 554, 19, 827, 615, 691, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 894, 31, 24, 895, 259, 45, 647, 554, 19, 834, 826, 691, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 138, 896, 897, 554, 19, 825, 826, 330, 867, 739, 138, 504, 557, 829, 29, 743, 696, 259, 741, 742, 579, 1384, 1141, 1385, 1386, 96, 1387, 1388, 253, 554, 19, 827, 615, 691, 669, 24, 1112, 1012, 21, 196, 444, 196, 392, 51, 1116, 226, 456, 259, 1389, 185, 186, 579, 1390, 279, 199, 51, 1391, 1392, 554, 19, 614, 615, 691, 669, 776, 1393, 61, 24, 207, 617, 24, 0, 207, 617, 194, 24, 681, 902, 185, 4, 61, 903, 412, 21, 22, 904, 905, 665, 68, 664, 0, 185, 196, 63, 906, 51, 907, 226, 908, 383, 909, 903, 412, 904, 51, 0, 185, 196, 910, 259, 911, 379, 412, 912, 182, 913, 51, 914, 321, 379, 51, 24, 206, 31, 915, 277, 301, 476, 916, 917, 0, 20, 33, 385, 386, 0, 387, 12, 163, 76, 164, 481, 165, 918, 480, 0, 388, 66, 919, 212])\n"
     ]
    }
   ],
   "source": [
    "#sanity check for a vocab_word\n",
    "sample_number = np.random.choice(100, 1)[0]\n",
    "vocab_word = vocab_id_list[sample_number]\n",
    "vocabID = vocab_dict[vocab_word]\n",
    "print(sample_number)\n",
    "print(vocab_word)\n",
    "assert vocabID == sample_number,  \"Vocab dictionary error\"\n",
    "print(\"\\n\" + \"--\"*10 + \"\\n\")\n",
    "\n",
    "#sanity check on the query content and doc content\n",
    "print(\"Sample of query content and doc content\")\n",
    "print(\"Query content:\\n\", queryID_to_content[0])\n",
    "print(\"\\nDoc sample:\\n\", docId_to_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(doc_id_list, open(\"doc_id_list.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using collected body from web urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to collect body from web urls and build doc content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_id_content = {} #dict maps from doc id to the contents in the doc. The content is saved as a list of vocab_ids\n",
    "\n",
    "vocab_dict = {} #mapping from vocab term to id \n",
    "vocab_id_list = [] #list where id maps to the vocab term (0 indexed)\n",
    "vocab_frequency = {} #number of times each vocab term appears in the vocab of documents\n",
    "\n",
    "\n",
    "for doc in doc_id_list:\n",
    "    try:\n",
    "        with urlopen(doc) as page:\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            body = soup.find('body')\n",
    "            title = soup.find('title')\n",
    "            body_content = []\n",
    "            title_content = []\n",
    "            content_list = [] #list containing all word IDs. To be attached to a doc\n",
    "\n",
    "            if body is not None:\n",
    "                body_content = body.text.split()\n",
    "\n",
    "            if title is not None:\n",
    "                title_content = title.text.split()\n",
    "\n",
    "            all_content = body_content + title_content\n",
    "            for word in all_content:\n",
    "                word = word.strip()\n",
    "                \n",
    "                if word.isalnum() == False:\n",
    "                    continue\n",
    "                \n",
    "                vocab_frequency[word] = vocab_frequency.get(word, 0) + 1\n",
    "\n",
    "                if vocab_dict.get(word, None) == None:\n",
    "                    vocab_id_list.append(word)\n",
    "                    vocab_dict[word] = len(vocab_id_list) - 1\n",
    "\n",
    "                word_id = vocab_dict[word]\n",
    "                content_list.append(word_id)\n",
    "            doc_id_content[doc_dict[doc]] = content_list\n",
    "    except:\n",
    "        doc_id_content[doc_dict[doc]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
